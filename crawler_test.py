import requests
from bs4 import BeautifulSoup
from langchain_core.documents import Document

# 1. è¨­å®šç›®æ¨™ç¶²å€ (å…¨åœ‹æ³•è¦è³‡æ–™åº« - å‹å‹•åŸºæº–æ³•)
url = "https://law.moj.gov.tw/LawClass/LawAll.aspx?PCode=N0030001"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

print(f"ğŸš€ é–‹å§‹çˆ¬å–é é¢ï¼š{url} ...")
response = requests.get(url, headers=headers)

# å…ˆæª¢æŸ¥ç‹€æ…‹ç¢¼ï¼Œç¢ºèªæœ‰æ²’æœ‰é€£ç·šæˆåŠŸ
print(f"ğŸ“¡ é€£ç·šç‹€æ…‹ç¢¼: {response.status_code}")

crawled_docs = []

if response.status_code == 200:
    print("âœ… é€£ç·šæˆåŠŸï¼é–‹å§‹è§£æ HTML...")
    soup = BeautifulSoup(response.text, "html.parser")

    # ã€é—œéµæ­¥é©Ÿ 1ã€‘é–å®šã€Œå¤§å¯¶ç®±ã€
    # æ ¹æ“šä½ çš„æˆªåœ–ï¼Œæ³•è¦å…§å®¹éƒ½åœ¨ class="law-reg-content" è£¡é¢
    law_content = soup.find(class_="law-reg-content")

    if law_content:
        # ã€é—œéµæ­¥é©Ÿ 2ã€‘æ‰¾å‡ºæ¯ä¸€æ¢æ³•è¦
        # æˆªåœ–é¡¯ç¤ºæ¯ä¸€æ¢éƒ½æ˜¯ä¸€å€‹ class="row" çš„ div
        # æˆ‘å€‘æ‰¾å‡ºæ‰€æœ‰åœ¨ law_content è£¡é¢çš„ row
        all_rows = law_content.find_all(class_="row")
        print(f"ğŸ” å…±ç™¼ç¾ {len(all_rows)} å€‹æ®µè½ (åŒ…å«æ¢æ–‡èˆ‡ç« ç¯€æ¨™é¡Œ)...\n")

        for row in all_rows:
            # ã€é—œéµæ­¥é©Ÿ 3ã€‘åˆ†é›¢æ¢è™Ÿèˆ‡å…§æ–‡
            # å·¦é‚Šï¼šclass="col-no" (æ¢è™Ÿ)
            col_no = row.find(class_="col-no")
            # å³é‚Šï¼šclass="col-data" (å…§æ–‡)
            col_data = row.find(class_="col-data")

            # åªæœ‰ç•¶ã€Œæ¢è™Ÿã€å’Œã€Œå…§æ–‡ã€åŒæ™‚å­˜åœ¨æ™‚ï¼Œæ‰ç®—æ˜¯ä¸€æ¢å®Œæ•´çš„æ³•è¦
            # (å› ç‚ºæœ‰æ™‚å€™ row è£¡é¢æ”¾çš„æ˜¯ "ç¬¬ ä¸€ ç«  ç¸½å‰‡" é€™ç¨®ç« ç¯€æ¨™é¡Œï¼Œå®ƒæ²’æœ‰ col-data)
            if col_no and col_data:
                article_no = col_no.get_text(strip=True)  # å–å¾— "ç¬¬ 1 æ¢"
                article_text = col_data.get_text(strip=True)  # å–å¾—å…§æ–‡

                # å°å‡ºä¾†æª¢æŸ¥çœ‹çœ‹
                print(f"ğŸ“Œ {article_no}")
                print(f"ğŸ“ {article_text[:50]}...")  # åªå°å‰50å­—
                print("-" * 20)

                # ã€é—œéµæ­¥é©Ÿ 4ã€‘å°è£æˆ Document
                # é€™è£¡æˆ‘å€‘åšä¸€å€‹å¾ˆæ£’çš„å„ªåŒ–ï¼šæŠŠæ¢è™Ÿç›´æ¥å¯«é€² Metadataï¼
                new_doc = Document(
                    page_content=f"{article_no}ï¼š{article_text}",  # å…§å®¹æ ¼å¼ï¼šç¬¬ 1 æ¢ï¼šå…§æ–‡...
                    metadata={
                        "source": "å‹å‹•åŸºæº–æ³•",
                        "url": url,
                        "article_id": article_no  # é€™æ¨£ä»¥å¾Œå¯ä»¥ç²¾æº–æœå°‹ "ç¬¬ 24 æ¢"
                    }
                )
                crawled_docs.append(new_doc)

        print(f"\nğŸ“¦ æˆåŠŸè½‰æ› {len(crawled_docs)} æ¢æ³•è¦ç‚º LangChain æ–‡ä»¶ç‰©ä»¶ï¼")

    else:
        print("âŒ æ‰¾ä¸åˆ° class='law-reg-content'ï¼Œå¯èƒ½æ˜¯ç¶²é æ”¹ç‰ˆäº†ï¼Ÿ")

else:
    print("âŒ ç¶²é è®€å–å¤±æ•—")