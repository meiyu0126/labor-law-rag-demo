import streamlit as st
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
from operator import itemgetter
import os

# 1. è¨­å®šé é¢
st.set_page_config(page_title="å‹žåŸºæ³• AI åŠ©æ‰‹", page_icon="âš–ï¸")
st.title("âš–ï¸ ä¼æ¥­å‹žåŸºæ³•æ™ºæ…§å•ç­”åŠ©æ‰‹")
st.caption("ðŸš€ Powered by Large Model ")


# 2. å»ºç«‹æˆ–è¼‰å…¥è³‡æ–™åº« (Persistence ç‰ˆæœ¬)
def build_vector_db_in_memory(file_path, embedding_function):
    # è¨­å®šè³‡æ–™åº«è¦å­˜åœ¨å“ªå€‹è³‡æ–™å¤¾ (è«‹ç¢ºä¿é€™å€‹è³‡æ–™å¤¾åç¨±æœ‰åœ¨ .gitignore è£¡)
    PERSIST_DIR = "chroma_db_data"

    # æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨
    if os.path.exists(PERSIST_DIR):
        print(f"--- [V19] ç™¼ç¾å·²å­˜åœ¨çš„è³‡æ–™åº« ({PERSIST_DIR})ï¼Œç›´æŽ¥è¼‰å…¥ï¼Œä¸æ‰£æ¬¾ï¼ ---")
        # ç›´æŽ¥è®€å–ç¡¬ç¢Ÿä¸Šçš„è³‡æ–™åº«
        db = Chroma(
            persist_directory=PERSIST_DIR,
            embedding_function=embedding_function,
            collection_name="labor_laws_v19_optimized"
        )
        return db

    # å¦‚æžœè³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œæ‰é–‹å§‹å»ºç«‹
    try:
        print(f"--- [V19] æ‰¾ä¸åˆ°è³‡æ–™åº«ï¼Œé–‹å§‹å»ºç«‹æ–°è³‡æ–™åº« (æœƒå‘¼å« OpenAI API)... ---")

        loader = PyPDFLoader(file_path)
        docs = loader.load()
        if not docs:
            print("âŒ éŒ¯èª¤: PDF å…§å®¹ç‚ºç©º")
            return None

        # åˆ‡åˆ†è¨­å®š
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=20,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ"]
        )
        chunks = text_splitter.split_documents(docs)

        # éŽæ¿¾é›œè¨Š
        clean_chunks = [c for c in chunks if len(c.page_content) > 150]

        # å»ºç«‹è³‡æ–™åº«ä¸¦æŒ‡å®šå„²å­˜è·¯å¾‘ (persist_directory)
        db = Chroma.from_documents(
            documents=clean_chunks,
            embedding=embedding_function,
            collection_name="labor_laws_v19_optimized",
            persist_directory=PERSIST_DIR  # <--- é—œéµï¼šå‘Šè¨´å®ƒå­˜åˆ°ç¡¬ç¢Ÿ
        )
        print("âœ… è³‡æ–™åº«å»ºç«‹ä¸¦å„²å­˜æˆåŠŸï¼")
        return db

    except Exception as e:
        print(f"âŒ å»ºç«‹å¤±æ•—: {e}")
        return None

# --- å»ºè­°æŠŠ format_chat_history æ¬åˆ°é€™è£¡ ---
def format_chat_history(messages):
    history_text = ""
    recent_messages = messages[-6:]
    for msg in recent_messages:
        if msg["role"] == "user":
            history_text += f"ä½¿ç”¨è€…: {msg['content']}\n"
        elif msg["role"] == "assistant":
            history_text += f"åŠ©æ‰‹: {msg['content']}\n"
    return history_text

# 3. è¼‰å…¥ç³»çµ±
@st.cache_resource(show_spinner=False)
def load_rag_system_v19():
    load_dotenv()
    FILE_PATH = os.path.join("data", "labor_law.pdf")

    embedding_function = OpenAIEmbeddings(model="text-embedding-3-large")

    db = build_vector_db_in_memory(FILE_PATH, embedding_function)
    if db is None: return None

    # ã€å„ªåŒ– 2 & 3ã€‘èª¿æ•´ MMR åƒæ•¸
    # lambda_mult=0.85: å¼·çƒˆè¦æ±‚ã€Œç›¸é—œæ€§ã€ï¼Œåªå…è¨±ä¸€é»žé»žã€Œå¤šæ¨£æ€§ã€ã€‚
    # k=4: åªå–å‰ 4 åï¼Œé¿å…ç¬¬ 5 åé–‹å§‹å‡ºç¾ä¸ç›¸é—œçš„é›œè¨Šã€‚
    retriever = db.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 4,
            "fetch_k": 20,
            "lambda_mult": 0.85
        }
    )

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

    #template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å‹žåŸºæ³•å•ç­”åŠ©æ‰‹ã€‚
    #è«‹å‹™å¿…ã€Œåªã€ä¾æ“šä»¥ä¸‹çš„ã€åƒè€ƒè³‡æ–™ã€‘ä¾†å›žç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

    #ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
    #{context}

    #ä½¿ç”¨è€…å•é¡Œï¼š{question}

    #å›žç­”ï¼š"""
    # ä¿®æ”¹å¾Œçš„ template (åŠ å…¥ {chat_history})
    template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å‹žåŸºæ³•å•ç­”åŠ©æ‰‹ã€‚
        è«‹ä¾æ“šã€åƒè€ƒè³‡æ–™ã€‘èˆ‡ã€æ­·å²å°è©±ã€‘ä¾†å›žç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

        ã€æ­·å²å°è©±ã€‘ï¼š
        {chat_history}

        ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
        {context}

        ä½¿ç”¨è€…å•é¡Œï¼š{question}

        å›žç­”ï¼š"""

    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # ä¿®æ”¹ Chain çš„è¼¸å…¥è™•ç†
    # é€™è£¡çš„æ„æ€æ˜¯ï¼š
    # 1. context: æ‹¿å­—å…¸è£¡çš„ "question" åŽ»åšæª¢ç´¢ (retriever)
    # 2. question: æ‹¿å­—å…¸è£¡çš„ "question" ç›´æŽ¥å‚³ä¸‹åŽ»
    # 3. chat_history: æ‹¿å­—å…¸è£¡çš„ "chat_history" ç›´æŽ¥å‚³ä¸‹åŽ»
    retrieval_step = RunnableParallel(
        {
            "context": itemgetter("question") | retriever,
            "question": itemgetter("question"),
            "chat_history": itemgetter("chat_history"),
        }
    )

    answer_step = (
            RunnablePassthrough.assign(context=lambda x: format_docs(x["context"]))
            | prompt
            | llm
            | StrOutputParser()
    )

    final_chain = retrieval_step | RunnableParallel({
        "response": answer_step,
        "context": lambda x: x["context"]
    })

    return final_chain


# 4. åˆå§‹åŒ– Session
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„å‹žåŸºæ³• AI åŠ©æ‰‹,è«‹è¼¸å…¥å‹žåŸºæ³•ç›¸é—œæŸ¥è©¢æˆ‘æœƒç›¡åŠ›ç‚ºä½ æä¾›èªªæ˜Žã€‚"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# 5. è¼‰å…¥ç³»çµ±
if "rag_chain" not in st.session_state:
    with st.spinner("ðŸš€ [V19] ç³»çµ±å„ªåŒ–ä¸­... æ­£åœ¨èª¿æ•´åˆ‡ç‰‡å¤§å°èˆ‡æ¬Šé‡..."):
        st.session_state.rag_chain = load_rag_system_v19()

rag_chain = st.session_state.rag_chain

# 6. è™•ç†è¼¸å…¥
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    if rag_chain:
        with st.chat_message("assistant"):
            with st.spinner("ðŸ” æ­£åœ¨é€²è¡Œç²¾æº–æª¢ç´¢..."):
                try:
                    # ---ã€é—œéµä¿®æ”¹é–‹å§‹ã€‘---

                    # 1. æ•´ç†æ­·å²ç´€éŒ„
                    history_str = format_chat_history(st.session_state.messages[:-1])  # æŽ’é™¤å‰›å‰›è¼¸å…¥çš„é‚£å¥

                    # 2. æ”¹æˆå‚³å…¥ã€Œå­—å…¸ã€ï¼ŒåŒ…å«å•é¡Œèˆ‡æ­·å²
                    result = rag_chain.invoke({
                        "question": prompt,
                        "chat_history": history_str
                    })

                    # ---ã€é—œéµä¿®æ”¹çµæŸã€‘---

                    response_text = result["response"]
                    source_docs = result["context"]
                    st.write(response_text)

                    if source_docs:
                        with st.expander("ðŸ“š æŸ¥çœ‹æœ€ä½³åƒè€ƒä¾†æº (Top 4 - Optimized)", expanded=True):
                            for i, doc in enumerate(source_docs):
                                try:
                                    page_idx = doc.metadata.get('page', 0)
                                    page_num = int(page_idx) + 1
                                except:
                                    page_num = "?"

                                source = os.path.basename(doc.metadata.get('source', 'Unknown'))
                                content = doc.page_content.replace('\n', ' ')

                                st.markdown(f"### ðŸ… ä¾†æº {i + 1}: ç¬¬ {page_num} é ")
                                st.info(content)

                    st.session_state.messages.append({"role": "assistant", "content": response_text})

                except Exception as e:
                    st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
    else:
        st.error("ç³»çµ±åˆå§‹åŒ–å¤±æ•—ã€‚")