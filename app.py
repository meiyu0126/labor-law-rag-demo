import streamlit as st
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import os

# 1. è¨­å®šé é¢æ¨™é¡Œ
st.set_page_config(page_title="å‹åŸºæ³• AI åŠ©æ‰‹", page_icon="âš–ï¸")
st.title("âš–ï¸ ä¼æ¥­å‹åŸºæ³•æ™ºæ…§å•ç­”åŠ©æ‰‹")
st.caption("ğŸš€ Powered by RAG (LangChain + ChromaDB + OpenAI)")


# 2. è¼‰å…¥ç’°å¢ƒèˆ‡è³‡æ–™åº« (åˆ©ç”¨ cache resource åŠ é€Ÿï¼Œä¸ç”¨æ¯æ¬¡é‡æ–°è®€å–)
@st.cache_resource
def load_rag_system():
    load_dotenv()
    CHROMA_PATH = "chroma_db"

    # æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦å­˜åœ¨
    if not os.path.exists(CHROMA_PATH):
        st.error("âŒ æ‰¾ä¸åˆ°å‘é‡è³‡æ–™åº«ï¼Œè«‹å…ˆåŸ·è¡Œ ingest.py å»ºç«‹è³‡æ–™åº«ï¼")
        return None

    embedding_function = OpenAIEmbeddings(model="text-embedding-3-small")
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)
    # é€™è£¡æˆ‘å€‘ç¶­æŒ k=5 çš„æˆåŠŸè¨­å®š
    retriever = db.as_retriever(search_kwargs={"k": 5})

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

    template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å‹åŸºæ³•å•ç­”åŠ©æ‰‹ã€‚
    è«‹ä¾æ“šä»¥ä¸‹çš„ã€åƒè€ƒè³‡æ–™ã€‘ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹ç›´æ¥èªªã€ŒæŠ±æ­‰ï¼Œæ ¹æ“šç›®å‰çš„è³‡æ–™åº«ï¼Œæˆ‘ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€ï¼Œä¸è¦è©¦åœ–æ†‘ç©ºæé€ ã€‚

    ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
    {context}

    ä½¿ç”¨è€…å•é¡Œï¼š{question}

    å›ç­”ï¼š"""

    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
    )

    return rag_chain


# åˆå§‹åŒ– RAG éˆ
rag_chain = load_rag_system()

# 3. è™•ç†å°è©±æ­·å² (Session State)
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„å‹åŸºæ³• AI åŠ©æ‰‹ã€‚è«‹å•é—œæ–¼åŠ ç­è²»ã€ä¼‘å‡æˆ–å·¥æ™‚ï¼Œæœ‰ä»€éº¼æƒ³å•çš„å—ï¼Ÿ"}]

# é¡¯ç¤ºæ­·å²è¨Šæ¯
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# 4. è™•ç†ä½¿ç”¨è€…è¼¸å…¥
if prompt := st.chat_input():
    # é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # ç”Ÿæˆ AI å›æ‡‰
    if rag_chain:
        with st.chat_message("assistant"):
            with st.spinner("ğŸ” æ­£åœ¨æª¢ç´¢æ³•è¦è³‡æ–™åº«..."):
                response = rag_chain.invoke(prompt)
                st.write(response)

        # å­˜å…¥æ­·å²ç´€éŒ„
        st.session_state.messages.append({"role": "assistant", "content": response})