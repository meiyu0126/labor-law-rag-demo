import streamlit as st
import os
import shutil
import tempfile  # <--- æ–°å¢žé€™å€‹
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv

# 1. è¨­å®šé é¢
st.set_page_config(page_title="å‹žåŸºæ³• AI åŠ©æ‰‹", page_icon="âš–ï¸")
st.title("âš–ï¸ ä¼æ¥­å‹žåŸºæ³•æ™ºæ…§å•ç­”åŠ©æ‰‹ (V8 - Tmp Dir Fix)")
st.caption("ðŸš€ Powered by RAG (Database stored in /tmp for Write Permissions)")


# 2. å®šç¾©å»ºç«‹è³‡æ–™åº«å‡½å¼
def build_vector_db(file_path, db_path, embedding_function):
    try:
        print(f"--- [V8] é–‹å§‹å»ºç«‹è³‡æ–™åº«: {db_path} ---")

        loader = PyPDFLoader(file_path)
        docs = loader.load()
        if not docs:
            st.error("âŒ éŒ¯èª¤: PDF å…§å®¹ç‚ºç©ºï¼Œè«‹æª¢æŸ¥ data/labor_law.pdf")
            return None

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ"]
        )
        chunks = text_splitter.split_documents(docs)
        st.write(f"ðŸ“„ æˆåŠŸè®€å– PDFï¼Œå…±åˆ‡åˆ†å‡º `{len(chunks)}` å€‹ç‰‡æ®µï¼Œæ­£åœ¨å¯«å…¥æš«å­˜å€è³‡æ–™åº«...")

        db = Chroma.from_documents(
            documents=chunks,
            embedding=embedding_function,
            persist_directory=db_path
        )
        print("âœ… è³‡æ–™åº«å»ºç«‹æˆåŠŸï¼")
        return db
    except Exception as e:
        # é¡¯ç¤ºæ›´è©³ç´°çš„éŒ¯èª¤è³‡è¨Š
        st.error(f"âŒ è³‡æ–™åº«å»ºç«‹å¤±æ•—: {e}")
        return None


# 3. è¼‰å…¥ RAG ç³»çµ±
def load_rag_system():
    load_dotenv()

    FILE_PATH = os.path.join("data", "labor_law.pdf")

    # ã€é—œéµä¿®æ”¹ã€‘ï¼šä½¿ç”¨ç³»çµ±æš«å­˜ç›®éŒ„ /tmp
    # é€™æ¨£å¯ä»¥ä¿è­‰æœ‰å¯«å…¥æ¬Šé™ï¼Œè§£æ±º readonly database å•é¡Œ
    TMP_DIR = tempfile.gettempdir()
    CHROMA_PATH = os.path.join(TMP_DIR, "chroma_db_v8_tmp")

    # é¡¯ç¤ºè·¯å¾‘çµ¦æ‚¨çœ‹ï¼Œç¢ºèªæ˜¯åœ¨ /tmp åº•ä¸‹
    print(f"Target Database Path: {CHROMA_PATH}")

    embedding_function = OpenAIEmbeddings(model="text-embedding-3-small")

    # å¼·åˆ¶åˆªé™¤èˆŠè³‡æ–™ (ç¢ºä¿æ¯æ¬¡éƒ½æ˜¯æ–°çš„)
    if os.path.exists(CHROMA_PATH):
        try:
            shutil.rmtree(CHROMA_PATH)
        except:
            pass

    # åŸ·è¡Œå»ºç«‹
    with st.spinner(f"ðŸ—ï¸ [V8] æ­£åœ¨æš«å­˜å€ ({CHROMA_PATH}) é‡å»ºè³‡æ–™åº«... (ç´„ 20 ç§’)"):
        db = build_vector_db(FILE_PATH, CHROMA_PATH, embedding_function)

    if db is None:
        return None

    # --- RAG Chain è¨­å®š ---
    retriever = db.as_retriever(search_kwargs={"k": 5})
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

    template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å‹žåŸºæ³•å•ç­”åŠ©æ‰‹ã€‚
    è«‹ä¾æ“šä»¥ä¸‹çš„ã€åƒè€ƒè³‡æ–™ã€‘ä¾†å›žç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    å¦‚æžœè³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹ç›´æŽ¥èªªã€ŒæŠ±æ­‰ï¼Œæ ¹æ“šç›®å‰çš„è³‡æ–™åº«ï¼Œæˆ‘ç„¡æ³•å›žç­”é€™å€‹å•é¡Œã€ã€‚

    ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
    {context}

    ä½¿ç”¨è€…å•é¡Œï¼š{question}

    å›žç­”ï¼š"""

    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    retrieval_step = RunnableParallel(
        {"context": retriever, "question": RunnablePassthrough()}
    )

    answer_step = (
            RunnablePassthrough.assign(context=lambda x: format_docs(x["context"]))
            | prompt
            | llm
            | StrOutputParser()
    )

    final_chain = retrieval_step | RunnableParallel({
        "response": answer_step,
        "context": lambda x: x["context"]
    })

    return final_chain


# 4. åˆå§‹åŒ– Session
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„å‹žåŸºæ³• AI åŠ©æ‰‹ (V8)ã€‚è«‹è¼¸å…¥ä½ æƒ³æŸ¥è©¢çš„å‹žåŸºæ³•å•é¡Œï¼š"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# 5. æ¯æ¬¡åŸ·è¡Œéƒ½è¼‰å…¥ç³»çµ±
rag_chain = load_rag_system()

# 6. è™•ç†ä½¿ç”¨è€…è¼¸å…¥
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    if rag_chain:
        with st.chat_message("assistant"):
            with st.spinner("ðŸ” æ­£åœ¨æª¢ç´¢æ³•è¦è³‡æ–™åº«..."):
                try:
                    result = rag_chain.invoke(prompt)
                    response_text = result["response"]
                    source_docs = result["context"]

                    st.write(response_text)

                    with st.expander("ðŸ“š æŸ¥çœ‹è³‡æ–™ä¾†æº (Source Documents)"):
                        if not source_docs:
                            st.info("æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„ä¾†æºæ–‡ä»¶ã€‚")
                        else:
                            for i, doc in enumerate(source_docs):
                                page = doc.metadata.get('page', 'Unknown')
                                source = os.path.basename(doc.metadata.get('source', 'Unknown'))
                                st.markdown(f"**ä¾†æº {i + 1}**: `{source}` (ç¬¬ {page} é )")
                                st.text(doc.page_content[:100] + "...")
                                st.divider()

                    st.session_state.messages.append({"role": "assistant", "content": response_text})

                except Exception as e:
                    st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")