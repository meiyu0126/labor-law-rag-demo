import streamlit as st
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import os

# 1. è¨­å®šé é¢
st.set_page_config(page_title="å‹åŸºæ³• AI åŠ©æ‰‹", page_icon="âš–ï¸")
st.title("âš–ï¸ ä¼æ¥­å‹åŸºæ³•æ™ºæ…§å•ç­”åŠ©æ‰‹")
st.caption("ğŸš€ Powered by RAG (LangChain + ChromaDB + OpenAI)")


# 2. è¼‰å…¥è³‡æ–™åº«
@st.cache_resource
def load_rag_system():
    load_dotenv()
    CHROMA_PATH = "chroma_db"

    if not os.path.exists(CHROMA_PATH):
        st.error("âŒ æ‰¾ä¸åˆ°å‘é‡è³‡æ–™åº«ï¼Œè«‹ç¢ºèªå·²åŸ·è¡Œ ingest.py ä¸¦å°‡ chroma_db ä¸Šå‚³è‡³ GitHubï¼")
        return None

    # ä½¿ç”¨èˆ‡ ingest.py ç›¸åŒçš„æ¨¡å‹
    embedding_function = OpenAIEmbeddings(model="text-embedding-3-small")
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

    # è¨­å®šæª¢ç´¢å™¨ (k=5)
    retriever = db.as_retriever(search_kwargs={"k": 5})

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

    template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å‹åŸºæ³•å•ç­”åŠ©æ‰‹ã€‚
    è«‹ä¾æ“šä»¥ä¸‹çš„ã€åƒè€ƒè³‡æ–™ã€‘ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹ç›´æ¥èªªã€ŒæŠ±æ­‰ï¼Œæ ¹æ“šç›®å‰çš„è³‡æ–™åº«ï¼Œæˆ‘ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€ã€‚

    ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
    {context}

    ä½¿ç”¨è€…å•é¡Œï¼š{question}

    å›ç­”ï¼š"""

    prompt = ChatPromptTemplate.from_template(template)

    # å®šç¾©æ ¼å¼åŒ–æ–‡ä»¶çš„å‡½å¼
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # --- ä¿®æ­£å¾Œçš„ RAG éˆçµé‚è¼¯ (æ›´ç©©å®š) ---

    # æ­¥é©Ÿ 1: å¹³è¡Œè™•ç† - ä¸€é‚Šå»æŠ“è³‡æ–™(context)ï¼Œä¸€é‚Šä¿ç•™ä½¿ç”¨è€…å•é¡Œ(question)
    retrieval_step = RunnableParallel(
        {"context": retriever, "question": RunnablePassthrough()}
    )

    # æ­¥é©Ÿ 2: ç”Ÿæˆå›ç­” - å°‡æŠ“åˆ°çš„è³‡æ–™æ ¼å¼åŒ–æˆå­—ä¸²ï¼Œç„¶å¾Œé¤µçµ¦ LLM
    answer_step = (
            RunnablePassthrough.assign(context=lambda x: format_docs(x["context"]))
            | prompt
            | llm
            | StrOutputParser()
    )

    # æ­¥é©Ÿ 3: çµ„åˆæœ€çµ‚è¼¸å‡º - å›å‚³ã€ŒAIå›ç­”ã€ä»¥åŠã€ŒåŸå§‹æ–‡ä»¶(ç”¨æ–¼é¡¯ç¤ºä¾†æº)ã€
    final_chain = retrieval_step | RunnableParallel({
        "response": answer_step,
        "context": lambda x: x["context"]
    })

    return final_chain


rag_chain = load_rag_system()

# 3. åˆå§‹åŒ–å°è©±
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„å‹åŸºæ³• AI åŠ©æ‰‹ã€‚è«‹è¼¸å…¥ä½ æƒ³æŸ¥è©¢çš„å‹åŸºæ³•å•é¡Œï¼š"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# 4. è™•ç†è¼¸å…¥
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    if rag_chain:
        with st.chat_message("assistant"):
            with st.spinner("ğŸ” æ­£åœ¨æª¢ç´¢æ³•è¦è³‡æ–™åº«..."):
                try:
                    # åŸ·è¡Œ RAG
                    result = rag_chain.invoke(prompt)

                    response_text = result["response"]
                    source_docs = result["context"]

                    # é¡¯ç¤ºå›ç­”
                    st.write(response_text)

                    # é¡¯ç¤ºè³‡æ–™ä¾†æº (Expander)
                    with st.expander("ğŸ“š æŸ¥çœ‹è³‡æ–™ä¾†æº (Source Documents)"):
                        if not source_docs:
                            st.info("æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„ä¾†æºæ–‡ä»¶ã€‚")
                        else:
                            for i, doc in enumerate(source_docs):
                                page = doc.metadata.get('page', 'Unknown')
                                source = os.path.basename(doc.metadata.get('source', 'Unknown'))
                                st.markdown(f"**ä¾†æº {i + 1}**: `{source}` (ç¬¬ {page} é )")
                                st.text(doc.page_content[:100] + "...")  # åªé¡¯ç¤ºå‰100å­—é è¦½
                                st.divider()

                    # æ›´æ–°ç´€éŒ„
                    st.session_state.messages.append({"role": "assistant", "content": response_text})

                except Exception as e:
                    st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")