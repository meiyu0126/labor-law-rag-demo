import streamlit as st
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import os

# 1. è¨­å®šé é¢æ¨™é¡Œ
st.set_page_config(page_title="å‹åŸºæ³• AI åŠ©æ‰‹", page_icon="âš–ï¸")
st.title("âš–ï¸ ä¼æ¥­å‹åŸºæ³•æ™ºæ…§å•ç­”åŠ©æ‰‹")
st.caption("ğŸš€ Powered by RAG (LangChain + ChromaDB + OpenAI)")


# 2. è¼‰å…¥ç’°å¢ƒèˆ‡è³‡æ–™åº« (åˆ©ç”¨ cache resource åŠ é€Ÿ)
@st.cache_resource
def load_rag_system():
    load_dotenv()
    CHROMA_PATH = "chroma_db"

    # æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦å­˜åœ¨
    if not os.path.exists(CHROMA_PATH):
        st.error("âŒ æ‰¾ä¸åˆ°å‘é‡è³‡æ–™åº«ï¼Œè«‹å…ˆåŸ·è¡Œ ingest.py å»ºç«‹è³‡æ–™åº«ï¼")
        return None

    embedding_function = OpenAIEmbeddings(model="text-embedding-3-small")
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

    # ä¿®æ”¹å¾Œçš„ retriever è¨­å®š
    retriever = db.as_retriever(
        search_type="similarity_score_threshold",  # 1. å•Ÿç”¨ã€Œé–€æª»éæ¿¾ã€æ¨¡å¼
        search_kwargs={
            "k": 5,  # æœ€å¤šé‚„æ˜¯æŠ“ 5 ç­†
            "score_threshold": 0.5  # 2. è¨­å®šé–€æª»ï¼šç›¸ä¼¼åº¦ä½æ–¼ 0.7 çš„ç›´æ¥ä¸Ÿæ‰
        }
    )


    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

    template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å‹åŸºæ³•å•ç­”åŠ©æ‰‹ã€‚
    è«‹ä¾æ“šä»¥ä¸‹çš„ã€åƒè€ƒè³‡æ–™ã€‘ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹ç›´æ¥èªªã€ŒæŠ±æ­‰ï¼Œæ ¹æ“šç›®å‰çš„è³‡æ–™åº«ï¼Œæˆ‘ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€ï¼Œä¸è¦è©¦åœ–æ†‘ç©ºæé€ ã€‚

    ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
    {context}

    ä½¿ç”¨è€…å•é¡Œï¼š{question}

    å›ç­”ï¼š"""

    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # === ğŸ”¥ é—œéµä¿®æ”¹é–‹å§‹ï¼šä½¿ç”¨ RunnableParallel ä¾†ä¿ç•™ä¾†æºæ–‡ä»¶ ===

    # 1. å…ˆå®šç¾©æª¢ç´¢æ­¥é©Ÿï¼šåŒæ™‚å–å¾—ã€Œæ–‡ä»¶(context)ã€å’Œã€Œå•é¡Œ(question)ã€
    retrieval_step = RunnableParallel(
        {"context": retriever, "question": RunnablePassthrough()}
    )

    # 2. å®šç¾©å›ç­”ç”Ÿæˆæ­¥é©Ÿï¼šæŠŠ context è½‰æˆå­—ä¸² -> ä¸Ÿçµ¦ Prompt -> LLM
    answer_step = (
            RunnablePassthrough.assign(context=lambda x: format_docs(x["context"]))
            | prompt
            | llm
            | StrOutputParser()
    )

    # 3. çµ„åˆæœ€çµ‚éˆï¼šåŒæ™‚å›å‚³ã€ŒåŸå§‹æ–‡ä»¶ (source_documents)ã€å’Œã€ŒAIå›ç­” (answer)ã€
    rag_chain = (
            retrieval_step
            | RunnableParallel({
        "source_documents": lambda x: x["context"],
        "answer": answer_step
    })
    )
    # === ğŸ”¥ é—œéµä¿®æ”¹çµæŸ ===

    return rag_chain


# åˆå§‹åŒ– RAG éˆ
rag_chain = load_rag_system()

# 3. è™•ç†å°è©±æ­·å² (Session State)
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„å‹åŸºæ³• AI åŠ©æ‰‹ã€‚è«‹å•é—œæ–¼åŠ ç­è²»ã€ä¼‘å‡æˆ–å·¥æ™‚ï¼Œæœ‰ä»€éº¼æƒ³å•çš„å—ï¼Ÿ"}]

# é¡¯ç¤ºæ­·å²è¨Šæ¯
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])
    # å¦‚æœæ­·å²è¨Šæ¯ä¸­æœ‰ä¾†æºè³‡è¨Šï¼Œä¹Ÿé¡¯ç¤ºå‡ºä¾† (å¯é¸)
    if "sources" in msg:
        with st.expander("æŸ¥çœ‹åƒè€ƒä¾†æº"):
            for source in msg["sources"]:
                st.markdown(f"- **{source['source']}** (Page {source['page']})")

# 4. è™•ç†ä½¿ç”¨è€…è¼¸å…¥
if prompt := st.chat_input():
    # é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # ç”Ÿæˆ AI å›æ‡‰
    if rag_chain:
        with st.chat_message("assistant"):
            with st.spinner("ğŸ” æ­£åœ¨æª¢ç´¢æ³•è¦è³‡æ–™åº«..."):
                # å‘¼å« invokeï¼Œç¾åœ¨ response æœƒæ˜¯ä¸€å€‹å­—å…¸ (Dictionary)
                result = rag_chain.invoke(prompt)

                answer = result["answer"]
                source_docs = result["source_documents"]

                # é¡¯ç¤ºå›ç­”
                st.write(answer)

                # === ğŸ”¥ æ–°å¢ï¼šé¡¯ç¤ºè³‡æ–™ä¾†æº ===
                # æ•´ç†ä¾†æºè³‡è¨Šï¼Œé¿å…é‡è¤‡é¡¯ç¤ºç›¸åŒçš„é æ•¸
                unique_sources = []
                seen_sources = set()

                for doc in source_docs:
                    # å–å¾—æª”å (å»é™¤è·¯å¾‘) å’Œé æ•¸
                    source_name = os.path.basename(doc.metadata.get("source", "æœªçŸ¥ä¾†æº"))
                    page_num = doc.metadata.get("page", 0) + 1  # ç¨‹å¼å¾0é–‹å§‹ï¼Œç¿’æ…£ä¸ŠåŠ 1é¡¯ç¤º

                    identifier = f"{source_name}-{page_num}"
                    if identifier not in seen_sources:
                        unique_sources.append({"source": source_name, "page": page_num})
                        seen_sources.add(identifier)

                # ä½¿ç”¨æŠ˜ç–Šå…ƒä»¶ (Expander) é¡¯ç¤ºä¾†æº
                with st.expander("ğŸ“š æŸ¥çœ‹è³‡æ–™ä¾†æº (Source Documents)"):
                    for item in unique_sources:
                        st.markdown(f"- ğŸ“„ **{item['source']}** : ç¬¬ {item['page']} é ")
                    st.caption("è¨»ï¼šé æ•¸ç‚º PDF åŸå§‹é ç¢¼")

        # å­˜å…¥æ­·å²ç´€éŒ„ (åŒ…å«ä¾†æºè³‡è¨Šï¼Œä»¥ä¾¿é‡æ–°æ•´ç†é é¢æ™‚ä¹Ÿèƒ½é¡¯ç¤º)
        st.session_state.messages.append({
            "role": "assistant",
            "content": answer,
            "sources": unique_sources
        })