import streamlit as st
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import os
import tempfile  # <--- æ–°å¢é€™å€‹æ¨¡çµ„ä¾†è™•ç†ä¸Šå‚³æª”æ¡ˆ

# 1. è¨­å®šé é¢
st.set_page_config(page_title="ä¼æ¥­æ™ºèƒ½å•ç­”åŠ©æ‰‹", page_icon="ğŸ“‚")
st.title("ğŸ“‚ ä¼æ¥­æ™ºèƒ½æ–‡ä»¶å•ç­”åŠ©æ‰‹ (V22 - Upload Support)")
st.caption("ğŸš€ Powered by Large Model + Custom PDF Upload")

# --- å´é‚Šæ¬„ï¼šæª”æ¡ˆä¸Šå‚³å€ ---
with st.sidebar:
    st.header("ğŸ“‚ æ–‡ä»¶ä¸Šå‚³")
    uploaded_file = st.file_uploader("è«‹ä¸Šå‚³æ‚¨çš„ PDF æ–‡ä»¶", type=["pdf"])

    st.divider()
    st.header("âš™ï¸ ç³»çµ±åƒæ•¸")
    st.info(f"Chunk Size: 1000")
    st.info(f"Chunk Overlap: 30")

    if uploaded_file:
        st.success(f"ç›®å‰ä½¿ç”¨æ–‡ä»¶ï¼š\n{uploaded_file.name}")
    else:
        st.warning("ç›®å‰ä½¿ç”¨é è¨­æ–‡ä»¶ï¼š\nå‹å‹•åŸºæº–æ³•.pdf")


# -------------------------

# 2. å»ºç«‹è³‡æ–™åº« (é›²ç«¯å®‰å…¨ç‰ˆ - In-Memory)
def build_vector_db_in_memory(file_path, embedding_function):
    try:
        # é¡¯ç¤ºè™•ç†ä¸­çš„æª”æ¡ˆåç¨±
        file_name = os.path.basename(file_path)
        print(f"--- [V22] é–‹å§‹è™•ç†æª”æ¡ˆ: {file_name} ---")

        loader = PyPDFLoader(file_path)
        docs = loader.load()
        if not docs:
            print("âŒ éŒ¯èª¤: PDF å…§å®¹ç‚ºç©º")
            return None

        # åˆ‡åˆ†è¨­å®š (ç¶­æŒæ‚¨çš„æœ€ä½³åƒæ•¸)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=30,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ"]
        )
        chunks = text_splitter.split_documents(docs)

        # éæ¿¾é›œè¨Š
        clean_chunks = [c for c in chunks if len(c.page_content) > 150]

        print(f"ğŸ“„ åˆ‡åˆ†å®Œæˆï¼Œå…± {len(clean_chunks)} ç­†æœ‰æ•ˆç‰‡æ®µ")

        # ä½¿ç”¨æª”æ¡ˆåç¨±ä¾†ä½œç‚º Collection Nameï¼Œç¢ºä¿ä¸åŒæª”æ¡ˆä¸æœƒæ··åœ¨ä¸€èµ·
        # é€™è£¡åšä¸€é»å­—ä¸²è™•ç†ï¼ŒæŠŠæª”åè®Šæˆåˆæ³•çš„ Collection Name (åªç•™è‹±æ•¸)
        import re
        safe_name = re.sub(r'[^a-zA-Z0-9]', '_', file_name)[:50]
        collection_name = f"rag_coll_{safe_name}"

        db = Chroma.from_documents(
            documents=clean_chunks,
            embedding=embedding_function,
            collection_name=collection_name
        )
        print("âœ… è³‡æ–™åº«å»ºç«‹æˆåŠŸ (è¨˜æ†¶é«”æ¨¡å¼)ï¼")
        return db

    except Exception as e:
        print(f"âŒ å»ºç«‹å¤±æ•—: {e}")
        return None


# 3. è¼‰å…¥ç³»çµ± (å¿«å–é‚è¼¯èª¿æ•´)
# é€™è£¡æˆ‘å€‘æŠŠ file_path ç•¶ä½œå¿«å–çš„ key
# åªè¦ file_path æ”¹è®Š (ä¾‹å¦‚ä½¿ç”¨è€…ä¸Šå‚³äº†æ–°æª”æ¡ˆ)ï¼Œå¿«å–å°±æœƒå¤±æ•ˆï¼Œè‡ªå‹•é‡å»º DB
@st.cache_resource(show_spinner=False)
def load_rag_system_v22(target_file_path):
    load_dotenv()

    embedding_function = OpenAIEmbeddings(model="text-embedding-3-large")

    db = build_vector_db_in_memory(target_file_path, embedding_function)
    if db is None: return None

    # ç¶­æŒæ‚¨çš„ MMR è¨­å®š
    retriever = db.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 4,
            "fetch_k": 20,
            "lambda_mult": 0.85
        }
    )

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡ä»¶å•ç­”åŠ©æ‰‹ã€‚
    è«‹ä¾æ“šã€åƒè€ƒè³‡æ–™ã€‘èˆ‡ã€æ­·å²å°è©±ã€‘ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

    ã€æ­·å²å°è©±ã€‘ï¼š
    {chat_history}

    ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
    {context}

    ä½¿ç”¨è€…å•é¡Œï¼š{question}

    å›ç­”ï¼š"""

    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    from operator import itemgetter

    retrieval_step = RunnableParallel(
        {
            "context": itemgetter("question") | retriever,
            "question": itemgetter("question"),
            "chat_history": itemgetter("chat_history"),
        }
    )

    answer_step = (
            RunnablePassthrough.assign(context=lambda x: format_docs(x["context"]))
            | prompt
            | llm
            | StrOutputParser()
    )

    final_chain = retrieval_step | RunnableParallel({
        "response": answer_step,
        "context": lambda x: x["context"]
    })

    return final_chain


# --- æ­·å²è¨Šæ¯è™•ç† ---
def format_chat_history(messages):
    history_text = ""
    recent_messages = messages[-6:]
    for msg in recent_messages:
        if msg["role"] == "user":
            history_text += f"ä½¿ç”¨è€…: {msg['content']}\n"
        elif msg["role"] == "assistant":
            history_text += f"åŠ©æ‰‹: {msg['content']}\n"
    return history_text


# 4. è™•ç†æª”æ¡ˆé‚è¼¯ (é—œéµæ­¥é©Ÿ)
if uploaded_file:
    # å¦‚æœä½¿ç”¨è€…æœ‰ä¸Šå‚³æª”æ¡ˆ
    # 1. å»ºç«‹ä¸€å€‹æš«å­˜æª”
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name
else:
    # å¦‚æœæ²’ä¸Šå‚³ï¼Œä½¿ç”¨é è¨­çš„å‹åŸºæ³•
    tmp_file_path = os.path.join("data", "labor_law.pdf")

# 5. åˆå§‹åŒ– Session
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ä½ å¥½ï¼è«‹ä¸Šå‚³ PDF æ–‡ä»¶ï¼Œæˆ–ç›´æ¥è©¢å•å‹åŸºæ³•ç›¸é—œå•é¡Œã€‚"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# 6. è¼‰å…¥ç³»çµ± (æ ¹æ“š tmp_file_path æ±ºå®šæ˜¯å¦é‡å»º)
if "rag_chain" not in st.session_state or st.session_state.get("current_file") != tmp_file_path:
    with st.spinner("ğŸš€ æ­£åœ¨åˆ†ææ–‡ä»¶ä¸¦å»ºç«‹çŸ¥è­˜åº«..."):
        # å‘¼å«å»ºåº«å‡½å¼
        chain = load_rag_system_v22(tmp_file_path)
        # å°‡ chain å­˜å…¥ session
        st.session_state.rag_chain = chain
        # è¨˜éŒ„ç›®å‰ä½¿ç”¨çš„æª”æ¡ˆè·¯å¾‘ï¼Œä»¥ä¾¿åµæ¸¬è®Šæ›´
        st.session_state.current_file = tmp_file_path

        # å¦‚æœæ˜¯åˆ‡æ›æª”æ¡ˆï¼Œå»ºè­°æ¸…ç©ºå°è©±ç´€éŒ„ï¼Œé¿å…æ··æ·† (å¯é¸)
        # st.session_state.messages = [{"role": "assistant", "content": "å·²åˆ‡æ›æ–‡ä»¶ï¼Œè«‹ç™¼å•ï¼"}]

rag_chain = st.session_state.rag_chain

# 7. è™•ç†è¼¸å…¥
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    if rag_chain:
        with st.chat_message("assistant"):
            with st.spinner("ğŸ” æ­£åœ¨æª¢ç´¢..."):
                try:
                    history_str = format_chat_history(st.session_state.messages[:-1])

                    result = rag_chain.invoke({
                        "question": prompt,
                        "chat_history": history_str
                    })

                    response_text = result["response"]
                    source_docs = result["context"]
                    st.write(response_text)

                    if source_docs:
                        with st.expander("ğŸ“š æŸ¥çœ‹æœ€ä½³åƒè€ƒä¾†æº", expanded=True):
                            for i, doc in enumerate(source_docs):
                                try:
                                    page_idx = doc.metadata.get('page', 0)
                                    page_num = int(page_idx) + 1
                                except:
                                    page_num = "?"

                                source = os.path.basename(doc.metadata.get('source', 'Unknown'))
                                content = doc.page_content.replace('\n', ' ')

                                st.markdown(f"### ğŸ… ä¾†æº {i + 1}: ç¬¬ {page_num} é ")
                                st.info(content)

                    st.session_state.messages.append({"role": "assistant", "content": response_text})

                except Exception as e:
                    st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
    else:
        st.error("ç³»çµ±åˆå§‹åŒ–å¤±æ•—ã€‚")