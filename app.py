import streamlit as st
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import os
import tempfile
import time
import requests
from bs4 import BeautifulSoup
from langchain_core.documents import Document

load_dotenv()
#å¾å…¨åœ‹æ³•è¦è³‡æ–™åº«æŠ“å–å‹å‹•åŸºæº–æ³•
# åŠ ä¸Šé€™è¡Œï¼ŒStreamlit æœƒæŠŠçˆ¬ä¸‹ä¾†çš„çµæœå­˜èµ·ä¾†ï¼Œä¸æœƒæ¯æ¬¡éƒ½é‡è·‘
@st.cache_data(ttl=3600) # ttl=3600 ä»£è¡¨å¿«å– 1 å°æ™‚å¾ŒéæœŸ
def fetch_labor_law_docs():
    # 1. è¨­å®šç›®æ¨™ç¶²å€ (å…¨åœ‹æ³•è¦è³‡æ–™åº« - å‹å‹•åŸºæº–æ³•)
    url = "https://law.moj.gov.tw/LawClass/LawAll.aspx?PCode=N0030001"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    print(f"ğŸš€ é–‹å§‹çˆ¬å–é é¢ï¼š{url} ...")
    response = requests.get(url, headers=headers)

    # å…ˆæª¢æŸ¥ç‹€æ…‹ç¢¼ï¼Œç¢ºèªæœ‰æ²’æœ‰é€£ç·šæˆåŠŸ
    print(f"ğŸ“¡ é€£ç·šç‹€æ…‹ç¢¼: {response.status_code}")

    crawled_docs = []

    if response.status_code == 200:
        print("âœ… é€£ç·šæˆåŠŸï¼é–‹å§‹è§£æ HTML...")
        soup = BeautifulSoup(response.text, "html.parser")

        #é–å®šã€Œå¤§å¯¶ç®±ã€
        law_content = soup.find(class_="law-reg-content")

        if law_content:
            #æ‰¾å‡ºæ¯ä¸€æ¢æ³•è¦
            all_rows = law_content.find_all(class_="row")
            print(f"ğŸ” å…±ç™¼ç¾ {len(all_rows)} å€‹æ®µè½ (åŒ…å«æ¢æ–‡èˆ‡ç« ç¯€æ¨™é¡Œ)...\n")

            for row in all_rows:
                #åˆ†é›¢æ¢è™Ÿèˆ‡å…§æ–‡
                col_no = row.find(class_="col-no")
                col_data = row.find(class_="col-data")
                #BeautifulSoup æœ€å¸¸ç”¨çš„æ–¹æ³• .get_text();å®ƒæœƒæŠŠ HTML æ¨™ç±¤ï¼ˆ<div>...</div>ï¼‰ä¸Ÿæ‰ï¼Œåªç•™ä¸‹è£¡é¢çš„å­—ã€‚
                #strip=True;åŠ äº† strip=Trueï¼šå®ƒæœƒè‡ªå‹•æŠŠå‰å¾Œçš„æ›è¡Œç¬¦è™Ÿ (\n) å’Œå¤šé¤˜ç©ºç™½åˆ‡é™¤ï¼Œè®Šæˆä¹¾æ·¨çš„
                if col_no and col_data:
                    article_no = col_no.get_text(strip=True)
                    article_text = col_data.get_text(strip=True)

                    #å°è£æˆ Document
                    new_doc = Document(
                        page_content=f"{article_no}ï¼š{article_text}",
                        metadata={
                            "source": "å‹å‹•åŸºæº–æ³•",
                            "url": url,
                            "article_id": article_no
                        }
                    )
                    crawled_docs.append(new_doc)

            print(f"\nğŸ“¦ æˆåŠŸè½‰æ› {len(crawled_docs)} æ¢æ³•è¦ç‚º LangChain æ–‡ä»¶ç‰©ä»¶ï¼")

            return crawled_docs

        else:
            print("âŒ æ‰¾ä¸åˆ° class='law-reg-content'ï¼Œå¯èƒ½æ˜¯ç¶²é æ”¹ç‰ˆäº†ï¼Ÿ")
            return []  # å¤±æ•—æ™‚å›å‚³ç©ºåˆ—è¡¨

    else:
        print("âŒ ç¶²é è®€å–å¤±æ•—")
        return []

#å¥—ä»¶åç¨±,æ¶æ§‹è§’è‰²,åŠŸèƒ½èªªæ˜ (Why do we need it?)
#langchain,ç¸½æŒ‡æ® (Orchestrator),é€™æ˜¯æ ¸å¿ƒæ¡†æ¶ã€‚å®ƒè² è²¬æŠŠ LLMã€è³‡æ–™åº«ã€æ–‡ä»¶è®€å–å™¨ä¸²æ¥èµ·ä¾†ã€‚å°±åƒ Java çš„ Spring Frameworkï¼Œè² è²¬ç®¡ç†æ•´å€‹æ‡‰ç”¨ç¨‹å¼çš„æµç¨‹ã€‚
#langchain-community,æ“´å……æ¨¡çµ„åº« (Extensions),LangChain åœ¨æœ€è¿‘çš„ç‰ˆæœ¬æ”¹ç‰ˆäº†ï¼Œå°‡ç¬¬ä¸‰æ–¹æ•´åˆ (Integrations) æ‹†åˆ†å‡ºä¾†ã€‚è¦ä½¿ç”¨å¤§å¤šæ•¸çš„å·¥å…· (å¦‚æ–‡ä»¶è¼‰å…¥å™¨ã€å·¥å…·ç®±) éƒ½éœ€è¦å®ƒã€‚
#langchain-openai,å¤§è…¦ä»‹é¢ (Model Interface),å°ˆé–€ç”¨ä¾†è·Ÿ OpenAI API (GPT-3.5/4o) å°æ¥çš„é©…å‹•ç¨‹å¼ã€‚
#chromadb,å‘é‡è³‡æ–™åº« (Vector Store),é€™æ˜¯ RAG çš„é•·æœŸè¨˜æ†¶ã€‚å®ƒå°‡æ–‡å­—è½‰æ›æˆå‘é‡ (Embeddings) ä¸¦å„²å­˜åœ¨æœ¬åœ°ç«¯ï¼Œè®“æˆ‘å€‘å¯ä»¥ç”¨ã€Œèªæ„ã€ä¾†æœå°‹è³‡æ–™ï¼Œè€Œä¸åƒ…åƒ…æ˜¯é—œéµå­—æ¯”å°ã€‚
#pypdf,è³‡æ–™è®€å–å™¨ (Parser),æˆ‘å€‘çš„ ETL å·¥å…·ã€‚ç”¨ä¾†å¾ PDF æª”æ¡ˆä¸­æå–ç´”æ–‡å­—ï¼Œè®“ç¨‹å¼èƒ½å¤ ã€Œè®€æ‡‚ã€å‹åŸºæ³•æ–‡ä»¶ã€‚
#tiktoken,è¨ˆé‡å–®ä½ (Tokenizer),é€™æ˜¯ OpenAI é–‹ç™¼çš„ Token è¨ˆç®—å™¨ã€‚æˆ‘å€‘ç”¨å®ƒä¾†è¨ˆç®—å­—æ•¸èˆ‡æˆæœ¬ï¼Œä¸¦ç¢ºä¿é€çµ¦ AI çš„æ–‡å­—é‡ä¸æœƒè¶…éå®ƒçš„ Context Window ä¸Šé™ã€‚
#python-dotenv,é‡‘é‘°ç®¡ç† (Config Manager),ç”¨ä¾†è®€å– .env æª”æ¡ˆä¸­çš„è¨­å®šã€‚é€™æ˜¯è³‡å®‰æœ€ä½³å¯¦è¸ï¼Œé¿å…æŠŠ API Key ç¡¬å¯«åœ¨ç¨‹å¼ç¢¼è£¡ (Hard-code)ã€‚
# 2. è¨­å®šé é¢
st.set_page_config(page_title="ä¼æ¥­æ™ºèƒ½å•ç­”åŠ©æ‰‹", page_icon="ğŸ“‚")
st.title("ğŸ“‚ ä¼æ¥­æ™ºèƒ½æ–‡ä»¶å•ç­”åŠ©æ‰‹")
st.caption("ğŸš€ Powered by Large Model")

# --- å´é‚Šæ¬„ ---
with st.sidebar:
    st.header("ğŸ“‚ æ–‡ä»¶ä¸Šå‚³")
    uploaded_file = st.file_uploader("è«‹ä¸Šå‚³æ‚¨çš„ PDF æ–‡ä»¶", type=["pdf"])

    st.divider()
    st.header("âš™ï¸ ç³»çµ±åƒæ•¸")
    st.info(f"Chunk Size: 600")
    st.info(f"Chunk Overlap: 30")
    st.info(f"Top-K: 2(Strict)") # é¡¯ç¤ºç›®å‰çš„è¨­å®š

    if uploaded_file:
        st.success(f"ç›®å‰ä½¿ç”¨æ–‡ä»¶ï¼š\n{uploaded_file.name}")
    else:
        st.warning("ç›®å‰ä½¿ç”¨é è¨­æ–‡ä»¶ï¼š\nå‹å‹•åŸºæº–æ³•")
# -------------------------

# 3. å»ºç«‹è³‡æ–™åº«(æ”¯æ´ PDF è·¯å¾‘ æˆ– Document åˆ—è¡¨)
def build_vector_db_in_memory(source_data, embedding_function, is_web_data=False,original_filename=None):
    """
        source_data: å¯ä»¥æ˜¯æª”æ¡ˆè·¯å¾‘ (str) æˆ–æ˜¯æ–‡ä»¶åˆ—è¡¨ (list)
        is_web_data: æ¨™è¨˜æ˜¯å¦ç‚ºç¶²è·¯çˆ¬èŸ²è³‡æ–™;is_web_data=true->ç¶²è·¯çˆ¬èŸ²è³‡æ–™
    """
    try:
        # --- åˆ†æ”¯ A: è™•ç† PDF æª”æ¡ˆ ---
        if not is_web_data:
            file_path = source_data
            #å¦‚æœæœ‰å‚³å…¥åŸå§‹æª”åï¼Œå°±ç”¨åŸå§‹æª”åï¼›å¦å‰‡ç”¨è·¯å¾‘æª”å
            file_name = original_filename if original_filename else os.path.basename(file_path)

            print(f"--- é–‹å§‹è™•ç† PDF æª”æ¡ˆ: {file_name} ---")

            loader = PyPDFLoader(file_path)
            docs = loader.load()
            if not docs:
                print("âŒ éŒ¯èª¤: PDF å…§å®¹ç‚ºç©º")
                return None

            # å¼·åˆ¶æŠŠ Metadata è£¡çš„ source æ”¹å›åŸå§‹æª”å
            # é€™æ¨£ UI é¡¯ç¤ºæ™‚ï¼Œæ‰æœƒæ˜¯ "88åå€‹ç«¥å¥³.pdf" è€Œä¸æ˜¯ "tmpxyz.pdf"
            if original_filename:
                for doc in docs:
                    doc.metadata['source'] = original_filename

            # PDF éœ€è¦åˆ‡åˆ† (Chunking)
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=600,
                chunk_overlap=30,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ"]
            )
            chunks = text_splitter.split_documents(docs)
        # --- åˆ†æ”¯ B: è™•ç†ç¶²è·¯çˆ¬èŸ²è³‡æ–™ ---
        else:
            print("--- é–‹å§‹è™•ç†ç¶²è·¯çˆ¬èŸ²è³‡æ–™ ---")
            docs = source_data  # source_data æ˜¯ List[Document]
            if not docs:
                print("âŒ éŒ¯èª¤: çˆ¬èŸ²è³‡æ–™ç‚ºç©º")
                return None
            # ç¶²è·¯çˆ¬èŸ²çš„è³‡æ–™æ¯ä¸€æ¢å°±æ˜¯ä¸€æ¢æ³•è¦ï¼Œé€šå¸¸ä¸éœ€è¦å†åˆ‡åˆ†ï¼Œæˆ–è€…ç°¡å–®åˆ‡åˆ†å³å¯
            # é€™è£¡æˆ‘å€‘ç›´æ¥æŠŠå®ƒç•¶ä½œ chunks ä½¿ç”¨ (å› ç‚ºæ¯ä¸€æ¢æ³•è¦é•·åº¦é©ä¸­)
            chunks = docs
            file_name = "web_labor_law"  # çµ¦å€‹å‡æª”å
        # --- å…±åŒæµç¨‹: éæ¿¾é›œè¨Šèˆ‡å»ºåº« ---
        # éæ¿¾å¤ªçŸ­çš„é›œè¨Š
        clean_chunks = [c for c in chunks if len(c.page_content) > 10]
        print(f"ğŸ“„ æœ‰æ•ˆç‰‡æ®µå…± {len(clean_chunks)} ç­†")

        #ä»¥ä¸‹é€™æ®µç¨‹å¼ç¢¼çš„ç›®çš„æ˜¯ç‚ºäº†çµ¦å‘é‡è³‡æ–™åº«ï¼ˆChromaDBï¼‰ç”¢ç”Ÿä¸€å€‹ ã€Œåˆæ³•ã€å®‰å…¨ä¸”çµ•å°å”¯ä¸€ã€ çš„ Collection åç¨±ã€‚
        # å› ç‚ºè³‡æ–™åº«å°æ–¼åç¨±çš„è¦å®šé€šå¸¸å¾ˆåš´æ ¼ï¼ˆä¾‹å¦‚ï¼šä¸èƒ½æœ‰ç©ºæ ¼ã€ä¸èƒ½æœ‰ç‰¹æ®Šç¬¦è™Ÿã€ä¸èƒ½å¤ªé•·ï¼‰ï¼Œä¸”æˆ‘å€‘ä¸å¸Œæœ›æ–°çš„æª”æ¡ˆè¦†è“‹æ‰èˆŠçš„æª”æ¡ˆï¼Œæ‰€ä»¥éœ€è¦é€™æ®µã€Œæ•´å½¢æ‰‹è¡“ã€ã€‚
        import re
        # re.sub(æ­£å‰‡è¡¨é”å¼, æ›¿æ›æˆä»€éº¼, ç›®æ¨™å­—ä¸²);Python çš„æ­£å‰‡è¡¨é”å¼å–ä»£åŠŸèƒ½
        #r'[^a-zA-Z0-9]';[]ï¼šä»£è¡¨å­—å…ƒé›†åˆ;^ï¼šä»£è¡¨ã€Œéã€ (Not);a-zA-Z0-9ï¼šä»£è¡¨æ‰€æœ‰è‹±æ–‡å¤§å°å¯«å­—æ¯èˆ‡æ•¸å­—
        #r'[^a-zA-Z0-9]':åªè¦ä¸æ˜¯è‹±æ–‡å­—æ¯æˆ–æ•¸å­—çš„å­—å…ƒï¼ˆåŒ…å«ä¸­æ–‡ã€ç©ºæ ¼ã€é»ã€æ‹¬è™Ÿï¼‰ï¼Œå…¨éƒ¨éƒ½æŠ“å‡ºä¾†
        #'_'ï¼šæŠŠæŠ“å‡ºä¾†çš„é‚£äº›ã€Œéæ³•å­—å…ƒã€ï¼Œå…¨éƒ¨æ›¿æ›æˆåº•ç·š _ã€‚
        #[:30]ï¼šå­—ä¸²åˆ‡ç‰‡ã€‚ä¸ç®¡æª”åå¤šé•·ï¼Œåªå–å‰ 30 å€‹å­—ã€‚é€™æ˜¯ç‚ºäº†é¿å…è¶…é ChromaDB çš„åç¨±é•·åº¦é™åˆ¶ï¼ˆé€šå¸¸é™åˆ¶ 63 å­—å…ƒï¼‰ã€‚
        #ç›®çš„ï¼šç¢ºä¿æª”ååªå‰©ä¸‹ ASCII å®‰å…¨å­—å…ƒï¼Œä¸æœƒè®“è³‡æ–™åº«å ±éŒ¯ã€‚ç¯„ä¾‹ï¼š å¦‚æœ file_name æ˜¯ "å‹åŸºæ³• V1.0.pdf",çµæœï¼š safe_name æœƒè®Šæˆ "____V1_0_pdf" (ä¸­æ–‡å’Œé»éƒ½è¢«è®Šåº•ç·šäº†)ã€‚
        safe_name = re.sub(r'[^a-zA-Z0-9]', '_', file_name)[:30]
        #time.time()ï¼šå–å¾—ç›®å‰çš„ Unix æ™‚é–“æˆ³è¨˜ï¼ˆå¾ 1970/1/1 åˆ°ç¾åœ¨ç¶“éçš„ç§’æ•¸ï¼‰ï¼Œä¾‹å¦‚ 1735118855.123ã€‚
        #int(...)ï¼šè½‰æˆæ•´æ•¸ï¼Œå»æ‰å°æ•¸é»ã€‚
        #ç›®çš„ï¼š ç¢ºä¿ ã€Œå”¯ä¸€æ€§ (Uniqueness)ã€ã€‚
        # å³ä½¿ä¸Šå‚³åŒä¸€å€‹æª”æ¡ˆ labor_law.pdf å…©æ¬¡ï¼Œå› ç‚ºæ™‚é–“ä¸åŒï¼Œç”¢ç”Ÿçš„ ID å°±æœƒä¸åŒï¼Œç³»çµ±å°±ä¸æœƒææ··æˆ–éŒ¯èª¤è¦†è“‹ã€‚
        unique_id = int(time.time())
        #çµ„è£æœ€çµ‚åç¨± (Assembly)
        #f"..."ï¼šPython çš„ f-string æ ¼å¼åŒ–å­—ä¸²ã€‚
        #çµæ§‹ï¼š å›ºå®šå‰ç¶´ (rag_) + æ¸…æ´—å¾Œçš„æª”å + æ™‚é–“æˆ³è¨˜ã€‚
        #ç›®çš„ï¼š ç”¢ç”Ÿä¸€å€‹äººé¡ç¨å¾®çœ‹å¾—æ‡‚ï¼ˆçŸ¥é“æ˜¯ RAG ç”¨çš„ï¼Œä¹Ÿå¤§æ¦‚çŸ¥é“æ˜¯å“ªå€‹æª”ï¼‰ï¼Œä¸”æ©Ÿå™¨çµ•å°è®€å¾—æ‡‚çš„ IDã€‚
        #ä¾‹å¦‚:rag_______2025__pdf_1735000000
        #ChromaDB å° Collection Name æœ‰åš´æ ¼çš„å‘½åè¦ç¯„ï¼ˆé€šå¸¸è¦æ±‚ç”±å­—æ¯æ•¸å­—æˆ–åº•ç·šçµ„æˆï¼Œä¸”é•·åº¦æœ‰é™åˆ¶ï¼‰ã€‚ æ­¤å¤–ï¼Œç‚ºäº†æ”¯æ´å¤šç‰ˆæœ¬ç®¡ç†æˆ–é¿å…åŒåæª”æ¡ˆè¡çªï¼ŒåŠ ä¸Štime.time() æ™‚é–“æˆ³è¨˜ï¼Œç¢ºä¿æ¯æ¬¡ä¸Šå‚³å»ºç«‹çš„è³‡æ–™åº«éƒ½æ˜¯ç¨ç«‹ä¸”å”¯ä¸€çš„å¯¦é«”ï¼Œé€™å¢åŠ äº†ç³»çµ±çš„ç©©å¥æ€§ã€‚
        collection_name = f"rag_{safe_name}_{unique_id}"

        db = Chroma.from_documents(
            documents=clean_chunks,
            embedding=embedding_function,
            collection_name=collection_name
        )
        print(f"âœ… è³‡æ–™åº«å»ºç«‹æˆåŠŸ (ID: {unique_id})ï¼")
        return db

    except Exception as e:
            print(f"âŒ å»ºç«‹å¤±æ•—: {e}")
            return None

# 4. è¼‰å…¥ç³»çµ±
@st.cache_resource(show_spinner=False)
# st.cache_resource æœƒè‡ªå‹•æª¢æŸ¥è¼¸å…¥åƒæ•¸ target_source (å³çˆ¬èŸ²æŠ“ä¸‹ä¾†çš„ Document åˆ—è¡¨) çš„å…§å®¹é›œæ¹Šå€¼ (Hash) æ˜¯å¦æ”¹è®Šã€‚
# è‹¥ fetch_labor_law_docs çš„ 1 å°æ™‚å¿«å–éæœŸ (Expire)ï¼Œç•¶ä½¿ç”¨è€…é€å‡ºæŸ¥è©¢ (Submit) æ™‚ï¼Œ
# ç³»çµ±æœƒå¼·åˆ¶é‡æ–°çˆ¬å–æœ€æ–°æ³•æ¢ã€‚è‹¥æ³•æ¢å…§å®¹æœ‰æ›´æ–°ï¼Œtarget_source å°±æœƒæ”¹è®Šï¼Œé€²è€Œè§¸ç™¼é€™è£¡é‡æ–°å»ºç«‹å‘é‡è³‡æ–™åº«ã€‚
def load_rag_system(target_source,is_web=False,original_filename=None):

    embedding_function = OpenAIEmbeddings(model="text-embedding-3-large")
    # å‘¼å«ä¿®æ”¹å¾Œçš„å»ºåº«å‡½å¼
    db = build_vector_db_in_memory(target_source, embedding_function, is_web_data=is_web,original_filename=original_filename)
    if db is None: return None

    # 1. k=2: åªå–å‰2å
    retriever = db.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 2,
            "fetch_k": 20,
            "lambda_mult": 0.80
        }
    )

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡ä»¶å•ç­”åŠ©æ‰‹ã€‚
    è«‹ä¾æ“šã€åƒè€ƒè³‡æ–™ã€‘èˆ‡ã€æ­·å²å°è©±ã€‘ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

    ã€æ­·å²å°è©±ã€‘ï¼š
    {chat_history}

    ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
    {context}

    ä½¿ç”¨è€…å•é¡Œï¼š{question}

    å›ç­”ï¼š"""

    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    from operator import itemgetter

    retrieval_step = RunnableParallel(
        {
            "context": itemgetter("question") | retriever ,
            "question": itemgetter("question"),
            "chat_history": itemgetter("chat_history"),
        }
    )

    answer_step = (
            RunnablePassthrough.assign(context=lambda x: format_docs(x["context"]))
            | prompt
            | llm
            | StrOutputParser()
    )
    # è¨­å®šæœ€çµ‚è¼¸å‡ºçš„å¹³è¡Œè™•ç† (Parallel Execution)ï¼š
    # 1. "response": è² è²¬ç”Ÿæˆå›ç­”
    #    - æ¥æ”¶æª¢ç´¢çµæœ -> æ ¼å¼åŒ–ç‚ºå­—ä¸² (String) -> çµ„è£ Prompt -> LLM æ¨è«–è¼¸å‡º
    # 2. "context": è² è²¬ä¿ç•™åŸå§‹è­‰æ“š
    #    - ç›´æ¥ä¿ç•™æª¢ç´¢åˆ°çš„åŸå§‹æ–‡ä»¶ç‰©ä»¶ (List[Document])ï¼Œç”¨æ–¼å‰ç«¯é¡¯ç¤ºä¾†æº(ä½¿å‰ç«¯èƒ½æ‹¿åˆ° metadataï¼ˆé ç¢¼ã€æª”åï¼‰)
    final_chain = retrieval_step | RunnableParallel({
        "response": answer_step,
        "context": lambda x: x["context"]
    })

    return final_chain


# --- æ­·å²è¨Šæ¯è™•ç† ---
def format_chat_history(messages):
    history_text = ""
    recent_messages = messages[-6:]
    for msg in recent_messages:
        if msg["role"] == "user":
            history_text += f"ä½¿ç”¨è€…: {msg['content']}\n"
        elif msg["role"] == "assistant":
            history_text += f"åŠ©æ‰‹: {msg['content']}\n"
    return history_text

# 5. åˆå§‹åŒ– Session
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ä½ å¥½ï¼è«‹ä¸Šå‚³ PDF æ–‡ä»¶ï¼Œæˆ–ç›´æ¥è©¢å•å‹åŸºæ³•ç›¸é—œå•é¡Œã€‚"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# æ±ºå®šè³‡æ–™ä¾†æº
target_source = None
is_web = False
current_file_id = "default_web" # ç”¨ä¾†è­˜åˆ¥æª”æ¡ˆæ˜¯å¦æœ‰è®Šæ›´
real_name = None #åˆå§‹åŒ–è®Šæ•¸

if uploaded_file:
    # å¦‚æœæœ‰ä¸Šå‚³æª”æ¡ˆï¼Œèµ° PDF æµç¨‹
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        target_source = tmp_file.name
        is_web = False
        current_file_id = uploaded_file.name
        #æŠ“å–ä½¿ç”¨è€…ä¸Šå‚³çš„åŸå§‹æª”å
        real_name = uploaded_file.name
else:
    # å¦‚æœæ²’ä¸Šå‚³ï¼Œèµ°ç¶²è·¯çˆ¬èŸ²æµç¨‹
    target_source = fetch_labor_law_docs()
    is_web = True
    current_file_id = "web_labor_law"

# 6. è¼‰å…¥ç³»çµ±
# åˆ¤æ–·æ˜¯å¦éœ€è¦é‡æ–°å»ºç«‹ (æª”æ¡ˆè®Šäº† OR ç³»çµ±é‚„æ²’åˆå§‹åŒ–)
if "rag_chain" not in st.session_state or st.session_state.get("current_file") != current_file_id:
    with st.spinner("ğŸš€ æ­£åœ¨å»ºç½®çŸ¥è­˜åº« (PDF/Web)..."):
        # å‚³å…¥ source å’Œ æ¨™è¨˜
        chain = load_rag_system(target_source, is_web=is_web, original_filename=real_name)

        st.session_state.rag_chain = chain
        st.session_state.current_file = current_file_id

rag_chain = st.session_state.rag_chain

# 7. è™•ç†è¼¸å…¥
#è³¦å€¼è¡¨é”å¼;python3.8ä¹‹å¾Œçš„åŠŸèƒ½,st.chat_input()å°‡å€¼è³¦äºˆprompt,ç”±ifåšæ¢ä»¶åˆ¤æ–·,åˆ¤æ–·promptæ˜¯å¦æœ‰å€¼,è‹¥æœ‰å‰‡ä¸ç‚ºNone
if prompt := st.chat_input():
    #ç•¶ä½¿ç”¨è€…è¼¸å…¥ä¸¦é€å‡ºæ™‚ï¼Œç¨‹å¼æœƒåšå…©ä»¶äº‹:
    #å°‡è¨Šæ¯å­˜å…¥ st.session_state.messages (List)ï¼Œé€™æ˜¯ç‚ºäº†è®“ AI æœ‰çŸ­æœŸè¨˜æ†¶
    st.session_state.messages.append({"role": "user", "content": prompt})
    #ä½¿ç”¨ st.chat_message("user").write(prompt) ç«‹å³å°‡è¨Šæ¯é¡¯ç¤ºåœ¨èŠå¤©è¦–çª—ä¸Š
    st.chat_message("user").write(prompt)

    if rag_chain:
        #è² è²¬å»ºç«‹UIå®¹å™¨,æœƒåœ¨ç•«é¢ä¸Šç•«å‡ºä¸€å€‹ã€Œå°è©±æ¡†å€åŸŸã€ï¼Œä¸¦åœ¨å·¦å´ï¼ˆé è¨­ï¼‰é¡¯ç¤ºä¸€å€‹æ©Ÿå™¨äººçš„é ­åƒ,ç¢ºä¿å¾ŒçºŒçš„è¼¸å‡ºéƒ½æ­¸é¡åœ¨ã€åŠ©æ‰‹ã€çš„å°è©±æ¡†å…§
        #å°±åƒæ˜¯æ¼«ç•«è£¡çš„ä¸€å€‹ã€Œå°è©±æ³¡æ³¡ã€ï¼Œä¸¦æ¨™ç¤ºé€™æ˜¯ã€ŒåŠ©æ‰‹ã€èªªçš„è©±
        #ç”Ÿå‘½é€±æœŸï¼š æ°¸ä¹…å­˜åœ¨ï¼ˆç›´åˆ°è¢«æ²å‹•æˆ–æ˜¯é‡æ–°æ•´ç†ï¼‰ã€‚
        with st.chat_message("assistant"):
            #è² è²¬æä¾›å³æ™‚å›é¥‹;
            #RAGçš„æª¢ç´¢èˆ‡ç”Ÿæˆéœ€è¦æ™‚é–“ï¼Œåˆ©ç”¨é€™å€‹æš«æ™‚æ€§çš„ Spinner å‘Šè¨´ä½¿ç”¨è€…ç³»çµ±æ­£åœ¨é‹ä½œï¼Œé¿å…ä½¿ç”¨è€…ä»¥ç‚ºç¶²é ç•¶æ©Ÿã€‚
            #ç•¶é‹ç®—çµæŸé›¢é–‹å…§å±¤ with å€å¡Šæ™‚ï¼ŒSpinner æœƒè‡ªå‹•æ¶ˆå¤±ï¼Œç„¡ç¸«åˆ‡æ›é¡¯ç¤ºæœ€çµ‚çš„å›ç­”æ–‡å­—
            #ç”Ÿå‘½é€±æœŸï¼š æš«æ™‚çš„(Temporary)
            with st.spinner("ğŸ” æ­£åœ¨æª¢ç´¢..."):
                try:
                    #åœ¨å‘¼å« AI å‰ï¼Œå…ˆæ•´ç†éå»çš„å°è©±ç´€éŒ„
                    # messages[:-1]ä»£è¡¨ä¸åŒ…å«æœ€æ–°çš„é€™å¥ï¼Œé¿å…é‡è¤‡ã€‚
                    history_str = format_chat_history(st.session_state.messages[:-1])
                    #resultæ˜¯rag_chainå‘¼å«.invoke()å¾ŒåŸ·è¡Œçš„çµæœ,ä»¥åŒ…å«question,chat_historyçš„å­—å…¸ç‚ºåƒæ•¸
                    #rag_chainæ˜¯åŸ·è¡Œload_rag_systemå¾Œå›å‚³çš„Chainå¯¦é«”,å…¶è¼¸å‡ºçµæ§‹ (Output Schema)åŒ…æ‹¬:1.response 2.context é€™2å€‹key
                    #responseçš„ç”¢ç”Ÿéç¨‹(LCEL(LangChain Expression Language)çš„ç®¡ç·šåŒ–)ï¼šè³‡æ–™(question,chat_history)å…ˆæµç¶“ retrieval_step å–å¾—è³‡è¨Šï¼Œå†å‚³éçµ¦ answer_step é€²è¡Œæ ¼å¼åŒ–(contextè½‰ç‚ºå­—ä¸²),ç”¢ç”Ÿpromptå‚³çµ¦LLMç”Ÿæˆå›è¦†
                    #contextå‰‡æ˜¯ç¶“éæª¢ç´¢ä¹‹å¾Œå¾—åˆ°çš„åŸå§‹è³‡æ–™
                    #resultæ˜¯rag_chainåŸ·è¡Œ.invoke()å¾Œçš„ç”¢å‡ºï¼Œçµæ§‹å°æ‡‰final_chainçš„å®šç¾©åŒ…æ‹¬äº†response,context
                    #geminiæä¾›çš„è¨»è§£å¦‚ä¸‹
                    # [Input]: æº–å‚™åƒæ•¸
                    # å°‡ "ç•¶å‰å•é¡Œ" èˆ‡ "æ­·å²ç´€éŒ„" æ‰“åŒ…æˆ Dictionaryï¼Œä½œç‚º invoke çš„è¼¸å…¥
                    # [Process]: åŸ·è¡Œ RAG éˆ
                    # rag_chain æ˜¯ç”± load_rag_system å»ºæ§‹å®Œæˆçš„ç‰©ä»¶ (å³ final_chain)
                    # [Output]: è§£æçµæœ
                    # result çš„çµæ§‹ç”± final_chain ä¸­çš„ RunnableParallel å®šç¾©ï¼š
                    # 1. result["response"]: ç¶“é retrieval_step (æª¢ç´¢) -> answer_step (ç”Ÿæˆ) å¾Œçš„ AI å›è¦†å­—ä¸²
                    # 2. result["context"]: ç¶“é retrieval_step æª¢ç´¢åˆ°çš„åŸå§‹ Document ç‰©ä»¶åˆ—è¡¨ (åŸå§‹è³‡æ–™)
                    result = rag_chain.invoke({
                        "question": prompt,
                        "chat_history": history_str
                    })

                    response_text = result["response"]
                    source_docs = result["context"]
                    st.write(response_text)

                    if source_docs:
                        with st.expander("ğŸ“š æŸ¥çœ‹æœ€ä½³åƒè€ƒä¾†æº (Top 2)", expanded=True):
                            for i, doc in enumerate(source_docs):
                                # --- æ™ºæ…§åˆ¤æ–·ä¾†æºé¡å‹ ---
                                # å¦‚æœæœ‰ 'article_id' ä»£è¡¨æ˜¯æ³•è¦æ¢æ–‡
                                if 'article_id' in doc.metadata:
                                    source_label = doc.metadata['article_id']  # é¡¯ç¤º "ç¬¬ 24 æ¢"
                                    #page_info = ""  # æ³•è¦ä¸éœ€è¦é ç¢¼
                                # å¦å‰‡å°±æ˜¯ PDFï¼Œé¡¯ç¤ºé ç¢¼
                                else:
                                    page_idx = doc.metadata.get('page', 0)
                                    source_label = f"ç¬¬ {int(page_idx) + 1} é "

                                source_name = os.path.basename(doc.metadata.get('source', 'Unknown'))
                                content = doc.page_content.replace('\n', ' ')

                                st.markdown(f"### ğŸ… ä¾†æº {i + 1}: {source_name} {source_label}")
                                st.info(content)
                    #å°‡response_textå­˜å…¥st.session_state.messagesåˆ—è¡¨;ç‚ºäº†è®“é€™å‰‡å›ç­”æˆç‚ºä¸‹ä¸€æ¬¡å‘¼å« format_chat_history æ™‚çš„ä¸€éƒ¨åˆ†ï¼Œå½¢æˆå®Œæ•´çš„å°è©±ä¸Šä¸‹æ–‡ (Context Loop)
                    st.session_state.messages.append({"role": "assistant", "content": response_text})

                except Exception as e:
                    st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
    else:
        st.error("ç³»çµ±åˆå§‹åŒ–å¤±æ•—ã€‚")