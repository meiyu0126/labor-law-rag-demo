import streamlit as st
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import os

# 1. è¨­å®šé é¢ (V11 - Clean UX)
st.set_page_config(page_title="å‹åŸºæ³• AI åŠ©æ‰‹", page_icon="âš–ï¸")
st.title("âš–ï¸ ä¼æ¥­å‹åŸºæ³•æ™ºæ…§å•ç­”åŠ©æ‰‹ (V11 - Best UX)")
st.caption("ğŸš€ Powered by RAG (High Precision & Clean Display)")


# 2. å®šç¾©å»ºç«‹è³‡æ–™åº«å‡½å¼ (ç´”é‚è¼¯)
def build_vector_db_in_memory(file_path, embedding_function):
    try:
        print(f"--- [V11] é–‹å§‹å»ºç«‹è¨˜æ†¶é«”è³‡æ–™åº« ---")

        loader = PyPDFLoader(file_path)
        docs = loader.load()
        if not docs:
            print("âŒ éŒ¯èª¤: PDF å…§å®¹ç‚ºç©º")
            return None

        # ã€å„ªåŒ– 1ã€‘ç¸®å° chunk_size å›åˆ° 500ï¼Œæå‡ç²¾æº–åº¦
        # é€™æ¨£å¯ä»¥é¿å…æŠŠä¸ç›¸é—œçš„æ³•æ¢ï¼ˆå¦‚ç”¢å‡ï¼‰è·ŸåŠ ç­è²»æ··åœ¨ä¸€èµ·
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,  # æ”¹å›è¼ƒå°çš„åˆ‡ç‰‡ï¼Œè®“å‘é‡æ›´ç²¾ç¢º
            chunk_overlap=150,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ"]
        )
        chunks = text_splitter.split_documents(docs)
        print(f"ğŸ“„ åˆ‡åˆ†å®Œæˆï¼Œå…± {len(chunks)} ç­†ç‰‡æ®µ")

        db = Chroma.from_documents(
            documents=chunks,
            embedding=embedding_function
        )
        print("âœ… è¨˜æ†¶é«”è³‡æ–™åº«å»ºç«‹æˆåŠŸï¼")
        return db
    except Exception as e:
        print(f"âŒ å»ºç«‹å¤±æ•—: {e}")
        return None


# 3. è¼‰å…¥ RAG ç³»çµ± (ä½¿ç”¨å¿«å–)
@st.cache_resource(show_spinner=False)
def load_rag_system_v11():
    load_dotenv()

    FILE_PATH = os.path.join("data", "labor_law.pdf")
    embedding_function = OpenAIEmbeddings(model="text-embedding-3-small")

    db = build_vector_db_in_memory(FILE_PATH, embedding_function)

    if db is None:
        return None

    # ã€å„ªåŒ– 2ã€‘AI è®€å– 10 ç­†ï¼Œç¢ºä¿çŸ¥è­˜å®Œæ•´
    retriever = db.as_retriever(search_kwargs={"k": 10})

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

    template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å‹åŸºæ³•å•ç­”åŠ©æ‰‹ã€‚
    è«‹å‹™å¿…ã€Œåªã€ä¾æ“šä»¥ä¸‹çš„ã€åƒè€ƒè³‡æ–™ã€‘ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    å›ç­”æ™‚ï¼Œè«‹å„ªå…ˆå¼•ç”¨å…·é«”çš„ã€Œæ³•æ¢æ¢è™Ÿã€ï¼ˆä¾‹å¦‚ï¼šæ ¹æ“šç¬¬ 24 æ¢...ï¼‰ã€‚

    ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
    {context}

    ä½¿ç”¨è€…å•é¡Œï¼š{question}

    å›ç­”ï¼š"""

    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    retrieval_step = RunnableParallel(
        {"context": retriever, "question": RunnablePassthrough()}
    )

    answer_step = (
            RunnablePassthrough.assign(context=lambda x: format_docs(x["context"]))
            | prompt
            | llm
            | StrOutputParser()
    )

    final_chain = retrieval_step | RunnableParallel({
        "response": answer_step,
        "context": lambda x: x["context"]
    })

    return final_chain


# 4. åˆå§‹åŒ– Session & è¼‰å…¥ç³»çµ±
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„å‹åŸºæ³• AI åŠ©æ‰‹ (V11)ã€‚è«‹è¼¸å…¥ä½ æƒ³æŸ¥è©¢çš„å‹åŸºæ³•å•é¡Œï¼š"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# 5. å‘¼å«è¼‰å…¥
if "rag_chain" not in st.session_state:
    with st.spinner("ğŸš€ ç³»çµ±å•Ÿå‹•ä¸­... æ­£åœ¨è¨˜æ†¶é«”ä¸­æ§‹å»ºé«˜ç²¾åº¦çŸ¥è­˜åº«..."):
        st.session_state.rag_chain = load_rag_system_v11()

rag_chain = st.session_state.rag_chain

# 6. è™•ç†è¼¸å…¥
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    if rag_chain:
        with st.chat_message("assistant"):
            with st.spinner("ğŸ” æ­£åœ¨æª¢ç´¢ä¸¦æ•´ç†æ³•è¦..."):
                try:
                    result = rag_chain.invoke(prompt)
                    response_text = result["response"]
                    source_docs = result["context"]

                    st.write(response_text)

                    # ã€å„ªåŒ– 3ã€‘æ”¹å–„ä½¿ç”¨è€…é«”é©— (UX)
                    with st.expander("ğŸ“š æŸ¥çœ‹æœ€ä½³åƒè€ƒä¾†æº (Top 4 Sources)", expanded=False):
                        if not source_docs:
                            st.info("æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„ä¾†æºæ–‡ä»¶ã€‚")
                        else:
                            # [é—œéµç­–ç•¥]ï¼šé›–ç„¶ AI è®€äº† 10 ç­†ï¼Œä½†æˆ‘å€‘åªé¡¯ç¤ºå‰ 4 ç­†çµ¦ä½¿ç”¨è€…çœ‹
                            # é€™æ¨£å¯ä»¥éæ¿¾æ‰å¾Œé¢æ’åè¼ƒä½ã€è¼ƒä¸ç›¸é—œçš„é›œè¨Š (å¦‚ç¬¬ 12 é )
                            top_k_display = 4

                            for i, doc in enumerate(source_docs[:top_k_display]):
                                try:
                                    page_num = int(doc.metadata.get('page', 0)) + 1
                                except:
                                    page_num = doc.metadata.get('page', 'Unknown')

                                source = os.path.basename(doc.metadata.get('source', 'Unknown'))

                                # æ•´ç†å…§æ–‡ï¼šå»é™¤å¤šé¤˜æ›è¡Œï¼Œè®“é–±è®€æ›´æµæš¢
                                clean_content = doc.page_content.replace('\n', ' ')

                                # ä½¿ç”¨é†’ç›®çš„æ¨™é¡Œæ ¼å¼
                                st.markdown(f"### ğŸ… ä¾†æº {i + 1}: ç¬¬ {page_num} é ")

                                # é¡¯ç¤ºå®Œæ•´å…§æ–‡ (ä¸æˆªæ–·)ï¼Œä½¿ç”¨å¼•ç”¨å€å¡Šæ ¼å¼
                                st.info(clean_content)

                    st.session_state.messages.append({"role": "assistant", "content": response_text})

                except Exception as e:
                    st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
    else:
        st.error("ç³»çµ±åˆå§‹åŒ–å¤±æ•—ï¼Œç„¡æ³•åŸ·è¡Œå›ç­”ã€‚")