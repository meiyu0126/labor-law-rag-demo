import streamlit as st
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from dotenv import load_dotenv
import os

# 1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# è¨­å®šç¶²é æ¨™é¡Œ
st.set_page_config(page_title="å‹åŸºæ³• AI åŠ©æ‰‹", page_icon="âš–ï¸")
st.title("âš–ï¸ ä¼æ¥­å‹åŸºæ³•æ™ºæ…§å•ç­”åŠ©æ‰‹")
st.caption("ğŸš€ Powered by RAG (LangChain + ChromaDB + OpenAI)")


# 2. è¼‰å…¥ç’°å¢ƒèˆ‡è³‡æ–™åº« (åˆ©ç”¨ cache resource åŠ é€Ÿ)
@st.cache_resource
def load_rag_system():
    CHROMA_PATH = "chroma_db"

    # æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦å­˜åœ¨
    if not os.path.exists(CHROMA_PATH):
        st.error("âŒ æ‰¾ä¸åˆ°å‘é‡è³‡æ–™åº«ï¼Œè«‹å…ˆåŸ·è¡Œ ingest.py å»ºç«‹è³‡æ–™åº«ï¼")
        return None

    # æº–å‚™ Embedding æ¨¡å‹ (è«‹ç¢ºèªé€™è£¡è·Ÿæ‚¨ ingest.py ç”¨çš„æ¨¡å‹åç¨±ä¸€è‡´)
    embedding_function = OpenAIEmbeddings(model="text-embedding-3-small")

    # è¼‰å…¥è³‡æ–™åº«
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

    # ã€å›æ­¸åŸå§‹è¨­å®šã€‘ï¼šåªè¨­å®š k=5ï¼Œä¸åŠ ä»»ä½•éæ¿¾é–€æª»
    retriever = db.as_retriever(search_kwargs={"k": 5})

    # è¨­å®š LLM
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

    # å»ºç«‹æ¨™æº–çš„å•ç­”éˆ (RetrievalQA)
    # é€™æ˜¯ LangChain å°è£å¥½çš„æ¨™æº–æµç¨‹ï¼Œå®ƒæœƒè‡ªå‹•æŠŠæª¢ç´¢åˆ°çš„æ–‡å­—å¡çµ¦ LLM
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True  # é›–ç„¶é€™è£¡è¨­ Trueï¼Œä½†æˆ‘å€‘åœ¨ä»‹é¢ä¸Šé¸æ“‡ä¸é¡¯ç¤ºå®ƒ
    )

    return qa_chain


# åˆå§‹åŒ–ç³»çµ±
qa_chain = load_rag_system()

# 3. èŠå¤©ä»‹é¢
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„å‹åŸºæ³• AI åŠ©æ‰‹ã€‚è«‹å•é—œæ–¼åŠ ç­è²»ã€ä¼‘å‡æˆ–å·¥æ™‚ï¼Œæœ‰ä»€éº¼æƒ³å•çš„å—ï¼Ÿ"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    # é¡¯ç¤ºä½¿ç”¨è€…è¼¸å…¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    if qa_chain:
        # å–å¾— AI å›ç­”
        with st.chat_message("assistant"):
            with st.spinner("æ­£åœ¨æª¢ç´¢å‹åŸºæ³•è¦..."):
                # å‘¼å« QA Chain
                response = qa_chain.invoke({"query": prompt})
                result = response["result"]

                # é¡¯ç¤ºå›ç­”
                st.write(result)

                # æ›´æ–°å°è©±ç´€éŒ„
                st.session_state.messages.append({"role": "assistant", "content": result})

                # ã€è¨»ã€‘ï¼šé€™è£¡æ•…æ„ä¸å¯«å‡º source documents çš„ç¨‹å¼ç¢¼
                # é€™æ¨£å°±æ¢å¾©åˆ°äº†æ‚¨èªªã€ŒåŸæœ¬æ­£ç¢ºã€æ™‚çš„ç‹€æ…‹