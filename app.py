#Streamlitæ˜¯ç›®å‰Pythonç•Œæœ€ç´…çš„å¿«é€Ÿæ¶ç«™å·¥å…·
import streamlit as st
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import os
import tempfile
import time
#å¥—ä»¶åç¨±,æ¶æ§‹è§’è‰²,åŠŸèƒ½èªªæ˜ (Why do we need it?)
#langchain,ç¸½æŒ‡æ® (Orchestrator),é€™æ˜¯æ ¸å¿ƒæ¡†æ¶ã€‚å®ƒè² è²¬æŠŠ LLMã€è³‡æ–™åº«ã€æ–‡ä»¶è®€å–å™¨ä¸²æ¥èµ·ä¾†ã€‚å°±åƒ Java çš„ Spring Frameworkï¼Œè² è²¬ç®¡ç†æ•´å€‹æ‡‰ç”¨ç¨‹å¼çš„æµç¨‹ã€‚
#langchain-community,æ“´å……æ¨¡çµ„åº« (Extensions),LangChain åœ¨æœ€è¿‘çš„ç‰ˆæœ¬æ”¹ç‰ˆäº†ï¼Œå°‡ç¬¬ä¸‰æ–¹æ•´åˆ (Integrations) æ‹†åˆ†å‡ºä¾†ã€‚è¦ä½¿ç”¨å¤§å¤šæ•¸çš„å·¥å…· (å¦‚æ–‡ä»¶è¼‰å…¥å™¨ã€å·¥å…·ç®±) éƒ½éœ€è¦å®ƒã€‚
#langchain-openai,å¤§è…¦ä»‹é¢ (Model Interface),å°ˆé–€ç”¨ä¾†è·Ÿ OpenAI API (GPT-3.5/4o) å°æ¥çš„é©…å‹•ç¨‹å¼ã€‚
#chromadb,å‘é‡è³‡æ–™åº« (Vector Store),é€™æ˜¯ RAG çš„é•·æœŸè¨˜æ†¶ã€‚å®ƒå°‡æ–‡å­—è½‰æ›æˆå‘é‡ (Embeddings) ä¸¦å„²å­˜åœ¨æœ¬åœ°ç«¯ï¼Œè®“æˆ‘å€‘å¯ä»¥ç”¨ã€Œèªæ„ã€ä¾†æœå°‹è³‡æ–™ï¼Œè€Œä¸åƒ…åƒ…æ˜¯é—œéµå­—æ¯”å°ã€‚
#pypdf,è³‡æ–™è®€å–å™¨ (Parser),æˆ‘å€‘çš„ ETL å·¥å…·ã€‚ç”¨ä¾†å¾ PDF æª”æ¡ˆä¸­æå–ç´”æ–‡å­—ï¼Œè®“ç¨‹å¼èƒ½å¤ ã€Œè®€æ‡‚ã€å‹åŸºæ³•æ–‡ä»¶ã€‚
#tiktoken,è¨ˆé‡å–®ä½ (Tokenizer),é€™æ˜¯ OpenAI é–‹ç™¼çš„ Token è¨ˆç®—å™¨ã€‚æˆ‘å€‘ç”¨å®ƒä¾†è¨ˆç®—å­—æ•¸èˆ‡æˆæœ¬ï¼Œä¸¦ç¢ºä¿é€çµ¦ AI çš„æ–‡å­—é‡ä¸æœƒè¶…éå®ƒçš„ Context Window ä¸Šé™ã€‚
#python-dotenv,é‡‘é‘°ç®¡ç† (Config Manager),ç”¨ä¾†è®€å– .env æª”æ¡ˆä¸­çš„è¨­å®šã€‚é€™æ˜¯è³‡å®‰æœ€ä½³å¯¦è¸ï¼Œé¿å…æŠŠ API Key ç¡¬å¯«åœ¨ç¨‹å¼ç¢¼è£¡ (Hard-code)ã€‚
# 1. è¨­å®šé é¢
st.set_page_config(page_title="ä¼æ¥­æ™ºèƒ½å•ç­”åŠ©æ‰‹", page_icon="ğŸ“‚")
st.title("ğŸ“‚ ä¼æ¥­æ™ºèƒ½æ–‡ä»¶å•ç­”åŠ©æ‰‹")
st.caption("ğŸš€ Powered by Large Model")

# --- å´é‚Šæ¬„ ---
with st.sidebar:
    st.header("ğŸ“‚ æ–‡ä»¶ä¸Šå‚³")
    uploaded_file = st.file_uploader("è«‹ä¸Šå‚³æ‚¨çš„ PDF æ–‡ä»¶", type=["pdf"])

    st.divider()
    st.header("âš™ï¸ ç³»çµ±åƒæ•¸")
    st.info(f"Chunk Size: 600")
    st.info(f"Chunk Overlap: 30")
    st.info(f"Top-K: 3 (Strict)") # é¡¯ç¤ºç›®å‰çš„è¨­å®š

    if uploaded_file:
        st.success(f"ç›®å‰ä½¿ç”¨æ–‡ä»¶ï¼š\n{uploaded_file.name}")
    else:
        st.warning("ç›®å‰ä½¿ç”¨é è¨­æ–‡ä»¶ï¼š\nå‹å‹•åŸºæº–æ³•.pdf")
# -------------------------

# 2. å»ºç«‹è³‡æ–™åº«
def build_vector_db_in_memory(file_path, embedding_function):
    try:
        file_name = os.path.basename(file_path)
        print(f"--- é–‹å§‹è™•ç†æª”æ¡ˆ: {file_name} ---")

        loader = PyPDFLoader(file_path)
        docs = loader.load()
        if not docs:
            print("âŒ éŒ¯èª¤: PDF å…§å®¹ç‚ºç©º")
            return None

        # åˆ‡åˆ†è¨­å®š
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=30,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ"]
        )
        chunks = text_splitter.split_documents(docs)

        # éæ¿¾é›œè¨Š
        clean_chunks = [c for c in chunks if len(c.page_content) > 50]
        # 1. ç¯©é¸å‡ºé•·åº¦ <= 50 çš„ç‰‡æ®µ (åŸæœ¬è¢«ä¸Ÿæ£„çš„éƒ¨åˆ†)
        noise_chunks = [c for c in chunks if len(c.page_content) <= 50]

        print(f"ğŸ” å…±ç™¼ç¾ {len(noise_chunks)} ç­†è¢«éæ¿¾çš„å…§å®¹ã€‚\n")
        print("ä»¥ä¸‹åˆ—å‡ºå‰ 5 ç­†ç¯„ä¾‹ä¾›æª¢æŸ¥ï¼š")
        print("=" * 40)

        # 2. åˆ—å°å‡ºä¾†æª¢æŸ¥ (ç‚ºäº†é¿å…æ´—ç‰ˆï¼Œé€™è£¡åªå…ˆå°å‰ 5 ç­†)
        for i, c in enumerate(noise_chunks[:5]):
            content = c.page_content.strip()  # å»é™¤å‰å¾Œç©ºç™½è®“é¡¯ç¤ºæ›´æ•´é½Š
            length = len(c.page_content)

            print(f"ã€è¢«éæ¿¾ç‰‡æ®µ #{i + 1}ã€‘ (é•·åº¦: {length})")
            print(f"å…§å®¹: {content}")
            print("-" * 20)

        print(f"ğŸ“„ åˆ‡åˆ†å®Œæˆï¼Œå…± {len(clean_chunks)} ç­†æœ‰æ•ˆç‰‡æ®µ")

        # ç”¢ç”Ÿå”¯ä¸€ ID
        import re
        safe_name = re.sub(r'[^a-zA-Z0-9]', '_', file_name)[:30]
        unique_id = int(time.time())
        collection_name = f"rag_{safe_name}_{unique_id}"

        db = Chroma.from_documents(
            documents=clean_chunks,
            embedding=embedding_function,
            collection_name=collection_name
        )
        print(f"âœ… è³‡æ–™åº«å»ºç«‹æˆåŠŸ (ID: {unique_id})ï¼")
        return db

    except Exception as e:
        print(f"âŒ å»ºç«‹å¤±æ•—: {e}")
        return None


# 3. è¼‰å…¥ç³»çµ±
@st.cache_resource(show_spinner=False)
def load_rag_system(target_file_path):
    load_dotenv()

    embedding_function = OpenAIEmbeddings(model="text-embedding-3-large")

    db = build_vector_db_in_memory(target_file_path, embedding_function)
    if db is None: return None

    # ã€é—œéµä¿®æ”¹ã€‘
    # 1. k=3: åªå–å‰ 3 åï¼Œç æ‰ç¬¬ 4 åä»¥å¾Œçš„é›œè¨Šã€‚
    # 2. lambda_mult=0.7: ç¨å¾®èª¿é«˜ç›¸ä¼¼åº¦æ¬Šé‡ï¼Œæ¸›å°‘å› ç‚ºã€Œè¿½æ±‚å¤šæ¨£ã€è€ŒæŠ“åˆ°é€€ä¼‘é‡‘çš„æƒ…æ³ã€‚
    retriever = db.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 3,
            "fetch_k": 20,
            "lambda_mult": 0.8
        }
    )

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡ä»¶å•ç­”åŠ©æ‰‹ã€‚
    è«‹ä¾æ“šã€åƒè€ƒè³‡æ–™ã€‘èˆ‡ã€æ­·å²å°è©±ã€‘ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

    ã€æ­·å²å°è©±ã€‘ï¼š
    {chat_history}

    ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
    {context}

    ä½¿ç”¨è€…å•é¡Œï¼š{question}

    å›ç­”ï¼š"""

    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    from operator import itemgetter

    retrieval_step = RunnableParallel(
        {
            "context": itemgetter("question") | retriever ,
            "question": itemgetter("question"),
            "chat_history": itemgetter("chat_history"),
        }
    )

    answer_step = (
            RunnablePassthrough.assign(context=lambda x: format_docs(x["context"]))
            | prompt
            | llm
            | StrOutputParser()
    )
    # è¨­å®šæœ€çµ‚è¼¸å‡ºçš„å¹³è¡Œè™•ç† (Parallel Execution)ï¼š
    # 1. "response": è² è²¬ç”Ÿæˆå›ç­”
    #    - æ¥æ”¶æª¢ç´¢çµæœ -> æ ¼å¼åŒ–ç‚ºå­—ä¸² (String) -> çµ„è£ Prompt -> LLM æ¨è«–è¼¸å‡º
    # 2. "context": è² è²¬ä¿ç•™åŸå§‹è­‰æ“š
    #    - ç›´æ¥ä¿ç•™æª¢ç´¢åˆ°çš„åŸå§‹æ–‡ä»¶ç‰©ä»¶ (List[Document])ï¼Œç”¨æ–¼å‰ç«¯é¡¯ç¤ºä¾†æº(ä½¿å‰ç«¯èƒ½æ‹¿åˆ° metadataï¼ˆé ç¢¼ã€æª”åï¼‰)
    final_chain = retrieval_step | RunnableParallel({
        "response": answer_step,
        "context": lambda x: x["context"]
    })

    return final_chain


# --- æ­·å²è¨Šæ¯è™•ç† ---
def format_chat_history(messages):
    history_text = ""
    recent_messages = messages[-6:]
    for msg in recent_messages:
        if msg["role"] == "user":
            history_text += f"ä½¿ç”¨è€…: {msg['content']}\n"
        elif msg["role"] == "assistant":
            history_text += f"åŠ©æ‰‹: {msg['content']}\n"
    return history_text



# 4. è™•ç†æª”æ¡ˆé‚è¼¯
if uploaded_file:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name
else:
    tmp_file_path = os.path.join("data", "labor_law.pdf")

# 5. åˆå§‹åŒ– Session
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ä½ å¥½ï¼è«‹ä¸Šå‚³ PDF æ–‡ä»¶ï¼Œæˆ–ç›´æ¥è©¢å•å‹åŸºæ³•ç›¸é—œå•é¡Œã€‚"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# 6. è¼‰å…¥ç³»çµ±
if "rag_chain" not in st.session_state or st.session_state.get("current_file") != tmp_file_path:
    with st.spinner("ğŸš€ æ­£åœ¨å„ªåŒ–æª¢ç´¢æ¨¡å‹..."):
        chain = load_rag_system(tmp_file_path)
        st.session_state.rag_chain = chain
        st.session_state.current_file = tmp_file_path
rag_chain = st.session_state.rag_chain

# 7. è™•ç†è¼¸å…¥
#è³¦å€¼è¡¨é”å¼;python3.8ä¹‹å¾Œçš„åŠŸèƒ½,st.chat_input()å°‡å€¼è³¦äºˆprompt,ç”±ifåšæ¢ä»¶åˆ¤æ–·,åˆ¤æ–·promptæ˜¯å¦æœ‰å€¼,è‹¥æœ‰å‰‡ä¸ç‚ºNone
if prompt := st.chat_input():
    #ç•¶ä½¿ç”¨è€…è¼¸å…¥ä¸¦é€å‡ºæ™‚ï¼Œç¨‹å¼æœƒåšå…©ä»¶äº‹:
    #å°‡è¨Šæ¯å­˜å…¥ st.session_state.messages (List)ï¼Œé€™æ˜¯ç‚ºäº†è®“ AI æœ‰çŸ­æœŸè¨˜æ†¶
    st.session_state.messages.append({"role": "user", "content": prompt})
    #ä½¿ç”¨ st.chat_message("user").write(prompt) ç«‹å³å°‡è¨Šæ¯é¡¯ç¤ºåœ¨èŠå¤©è¦–çª—ä¸Š
    st.chat_message("user").write(prompt)

    if rag_chain:
        #è² è²¬å»ºç«‹UIå®¹å™¨,æœƒåœ¨ç•«é¢ä¸Šç•«å‡ºä¸€å€‹ã€Œå°è©±æ¡†å€åŸŸã€ï¼Œä¸¦åœ¨å·¦å´ï¼ˆé è¨­ï¼‰é¡¯ç¤ºä¸€å€‹æ©Ÿå™¨äººçš„é ­åƒ,ç¢ºä¿å¾ŒçºŒçš„è¼¸å‡ºéƒ½æ­¸é¡åœ¨ã€åŠ©æ‰‹ã€çš„å°è©±æ¡†å…§
        #å°±åƒæ˜¯æ¼«ç•«è£¡çš„ä¸€å€‹ã€Œå°è©±æ³¡æ³¡ã€ï¼Œä¸¦æ¨™ç¤ºé€™æ˜¯ã€ŒåŠ©æ‰‹ã€èªªçš„è©±
        #ç”Ÿå‘½é€±æœŸï¼š æ°¸ä¹…å­˜åœ¨ï¼ˆç›´åˆ°è¢«æ²å‹•æˆ–æ˜¯é‡æ–°æ•´ç†ï¼‰ã€‚
        with st.chat_message("assistant"):
            #è² è²¬æä¾›å³æ™‚å›é¥‹;
            #RAGçš„æª¢ç´¢èˆ‡ç”Ÿæˆéœ€è¦æ™‚é–“ï¼Œåˆ©ç”¨é€™å€‹æš«æ™‚æ€§çš„ Spinner å‘Šè¨´ä½¿ç”¨è€…ç³»çµ±æ­£åœ¨é‹ä½œï¼Œé¿å…ä½¿ç”¨è€…ä»¥ç‚ºç¶²é ç•¶æ©Ÿã€‚
            #ç•¶é‹ç®—çµæŸé›¢é–‹å…§å±¤ with å€å¡Šæ™‚ï¼ŒSpinner æœƒè‡ªå‹•æ¶ˆå¤±ï¼Œç„¡ç¸«åˆ‡æ›é¡¯ç¤ºæœ€çµ‚çš„å›ç­”æ–‡å­—
            #ç”Ÿå‘½é€±æœŸï¼š æš«æ™‚çš„(Temporary)
            with st.spinner("ğŸ” æ­£åœ¨æª¢ç´¢..."):
                try:
                    #åœ¨å‘¼å« AI å‰ï¼Œå…ˆæ•´ç†éå»çš„å°è©±ç´€éŒ„
                    # messages[:-1]ä»£è¡¨ä¸åŒ…å«æœ€æ–°çš„é€™å¥ï¼Œé¿å…é‡è¤‡ã€‚
                    history_str = format_chat_history(st.session_state.messages[:-1])
                    #resultæ˜¯rag_chainå‘¼å«.invoke()å¾ŒåŸ·è¡Œçš„çµæœ,ä»¥åŒ…å«question,chat_historyçš„å­—å…¸ç‚ºåƒæ•¸
                    #rag_chainæ˜¯åŸ·è¡Œload_rag_systemå¾Œå›å‚³çš„Chainå¯¦é«”,å…¶è¼¸å‡ºçµæ§‹ (Output Schema)åŒ…æ‹¬:1.response 2.context é€™2å€‹key
                    #responseçš„ç”¢ç”Ÿéç¨‹(LCEL(LangChain Expression Language)çš„ç®¡ç·šåŒ–)ï¼šè³‡æ–™(question,chat_history)å…ˆæµç¶“ retrieval_step å–å¾—è³‡è¨Šï¼Œå†å‚³éçµ¦ answer_step é€²è¡Œæ ¼å¼åŒ–(contextè½‰ç‚ºå­—ä¸²),ç”¢ç”Ÿpromptå‚³çµ¦LLMç”Ÿæˆå›è¦†
                    #contextå‰‡æ˜¯ç¶“éæª¢ç´¢ä¹‹å¾Œå¾—åˆ°çš„åŸå§‹è³‡æ–™
                    #resultæ˜¯rag_chainåŸ·è¡Œ.invoke()å¾Œçš„ç”¢å‡ºï¼Œçµæ§‹å°æ‡‰final_chainçš„å®šç¾©åŒ…æ‹¬äº†response,context
                    #geminiæä¾›çš„è¨»è§£å¦‚ä¸‹
                    # [Input]: æº–å‚™åƒæ•¸
                    # å°‡ "ç•¶å‰å•é¡Œ" èˆ‡ "æ­·å²ç´€éŒ„" æ‰“åŒ…æˆ Dictionaryï¼Œä½œç‚º invoke çš„è¼¸å…¥
                    # [Process]: åŸ·è¡Œ RAG éˆ
                    # rag_chain æ˜¯ç”± load_rag_system å»ºæ§‹å®Œæˆçš„ç‰©ä»¶ (å³ final_chain)
                    # [Output]: è§£æçµæœ
                    # result çš„çµæ§‹ç”± final_chain ä¸­çš„ RunnableParallel å®šç¾©ï¼š
                    # 1. result["response"]: ç¶“é retrieval_step (æª¢ç´¢) -> answer_step (ç”Ÿæˆ) å¾Œçš„ AI å›è¦†å­—ä¸²
                    # 2. result["context"]: ç¶“é retrieval_step æª¢ç´¢åˆ°çš„åŸå§‹ Document ç‰©ä»¶åˆ—è¡¨ (åŸå§‹è³‡æ–™)
                    result = rag_chain.invoke({
                        "question": prompt,
                        "chat_history": history_str
                    })

                    response_text = result["response"]
                    source_docs = result["context"]
                    st.write(response_text)

                    if source_docs:
                        #st.expander:ä½¿ç”¨ Streamlit çš„æ‘ºç–Šå…ƒä»¶ä¾†æ”¶ç´ä¾†æºè³‡æ–™
                        #expanded=True:è¨­å®š st.expanderçš„ã€Œé è¨­ç‹€æ…‹ã€ç‚ºå±•é–‹
                        with st.expander("ğŸ“š æŸ¥çœ‹æœ€ä½³åƒè€ƒä¾†æº (Top 3)", expanded=True):
                            for i, doc in enumerate(source_docs):
                                try:
                                    #å–å¾—é ç¢¼é‚è¼¯
                                    page_idx = doc.metadata.get('page', 0)
                                    page_num = int(page_idx) + 1
                                except:
                                    page_num = "?"

                                source = os.path.basename(doc.metadata.get('source', 'Unknown'))
                                #å»é™¤PDFåˆ‡åˆ†æ™‚ç”¢ç”Ÿçš„å¤šé¤˜æ›è¡Œç¬¦è™Ÿï¼Œè®“æ–‡å­—åœ¨UIä¸Šçš„é–±è®€é«”é©—æ›´æµæš¢ã€‚
                                content = doc.page_content.replace('\n', ' ')

                                st.markdown(f"### ğŸ… ä¾†æº {i + 1}: ç¬¬ {page_num} é ")
                                st.info(content)
                    #å°‡response_textå­˜å…¥st.session_state.messagesåˆ—è¡¨;ç‚ºäº†è®“é€™å‰‡å›ç­”æˆç‚ºä¸‹ä¸€æ¬¡å‘¼å« format_chat_history æ™‚çš„ä¸€éƒ¨åˆ†ï¼Œå½¢æˆå®Œæ•´çš„å°è©±ä¸Šä¸‹æ–‡ (Context Loop)
                    st.session_state.messages.append({"role": "assistant", "content": response_text})

                except Exception as e:
                    st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
    else:
        st.error("ç³»çµ±åˆå§‹åŒ–å¤±æ•—ã€‚")