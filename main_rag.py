import os
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
#å°‡æª¢ç´¢åˆ°çš„é€™äº›ç‰‡æ®µï¼ˆContextï¼‰é¤µçµ¦ GPTï¼Œçœ‹å®ƒèƒ½ä¸èƒ½å›ç­”å‡ºä¾†ã€‚
# 1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

CHROMA_PATH = "chroma_db"


def format_docs(docs):
    """å°‡æª¢ç´¢åˆ°çš„å¤šå€‹ç‰‡æ®µåˆä½µæˆä¸€æ®µæ–‡å­—"""
    return "\n\n".join(doc.page_content for doc in docs)


def run_rag_system():
    print("ğŸ¤– åˆå§‹åŒ– RAG ç³»çµ±ä¸­...")

    # 2. æº–å‚™æª¢ç´¢å™¨ (Retriever)
    embedding_function = OpenAIEmbeddings(model="text-embedding-3-large")
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

    # è¨­å®šæª¢ç´¢å™¨ï¼šåªæŠ“æœ€ç›¸é—œçš„å‰ 5 ç­† (ç‚ºäº†æé«˜æŠ“åˆ°ç¬¬ 24 æ¢çš„æ©Ÿç‡ï¼Œæˆ‘å€‘æŠŠ k æé«˜åˆ° 5)
    retriever = db.as_retriever(search_kwargs={"k": 5})

    # 3. æº–å‚™ LLM (å¤§è…¦)
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    # 4. è¨­è¨ˆ Prompt (æŒ‡ä»¤)
    # é€™ä¸€æ­¥æœ€é—œéµï¼æˆ‘å€‘è¦å‘Šè¨´ AIï¼šã€Œä½ åªèƒ½æ ¹æ“šæˆ‘çµ¦ä½ çš„è³‡æ–™å›ç­”ï¼Œä¸è¦çæ°ã€‚ã€
    template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å‹åŸºæ³•å•ç­”åŠ©æ‰‹ã€‚
    è«‹ä¾æ“šä»¥ä¸‹çš„ã€åƒè€ƒè³‡æ–™ã€‘ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹ç›´æ¥èªªã€ŒæŠ±æ­‰ï¼Œæ ¹æ“šç›®å‰çš„è³‡æ–™åº«ï¼Œæˆ‘ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€ï¼Œä¸è¦è©¦åœ–æ†‘ç©ºæé€ ã€‚

    ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
    {context}

    ä½¿ç”¨è€…å•é¡Œï¼š{question}

    å›ç­”ï¼š"""

    prompt = ChatPromptTemplate.from_template(template)

    # 5. å»ºç«‹ RAG éˆ (Chain)
    # é€™æ˜¯ LangChain çš„ LCEL èªæ³• (LangChain Expression Language)
    # æµç¨‹ï¼šå–å¾—å•é¡Œ -> æª¢ç´¢è³‡æ–™ -> æ•´ç†è³‡æ–™ -> å¡«å…¥ Prompt -> ä¸Ÿçµ¦ LLM -> è§£æå­—ä¸²
    #åˆ†æ”¯ä¸€ "question": RunnablePassthrough()ï¼šRunnablePassthrough()æ˜¯ä¸€å€‹ã€Œç›´é€šè»Šã€,æŠŠ question åŸå°ä¸å‹•åœ°å‚³çµ¦ "question" é€™å€‹ Key
    #åˆ†æ”¯äºŒ "context": retriever | format_docsï¼š
    #è¼¸å…¥çš„ "åŠ ç­è²»æ€éº¼ç®—ï¼Ÿ" å…ˆå‚³çµ¦ retrieverï¼ˆæª¢ç´¢å™¨ï¼‰ï¼Œæ‰¾å‡ºç›¸é—œçš„ Document ç‰©ä»¶åˆ—è¡¨
    #æ¥è‘—æŠŠé€™äº› Documents å‚³çµ¦ format_docs å‡½å¼ï¼ˆé€šå¸¸æ˜¯ç”¨ \n\n æ¥èµ·ä¾†ï¼‰ï¼Œè½‰æˆä¸€å€‹é•·å­—ä¸²ã€‚
    #æœ€å¾Œé€™å€‹é•·å­—ä¸²è¢«å‚³çµ¦ "context" é€™å€‹ Keyã€‚
    #æ‰€ä»¥rag_chainçš„ç”¢ç”Ÿ:1.ç”¢ç”Ÿkeyç‚ºcontext,questionçš„å­—å…¸
    # 2.LangChain æœƒè‡ªå‹•æŠŠå­—å…¸è£¡çš„ context å’Œ question å¡«å…¥ Prompt Template å°æ‡‰çš„ {context} èˆ‡ {question} é ç•™ä½ç½®ä¸­ã€‚
    #ç”¢å‡ºï¼šä¸€å€‹å®Œæ•´çš„ PromptValue ç‰©ä»¶,åŒ…æ‹¬äº†System message("ä½ æ˜¯ä¸€å€‹å°ˆæ¥­åŠ©æ‰‹..."),User Message (æœ‰ "æ–‡ä»¶å…§å®¹" + "å•é¡Œ")ã€‚
    #å‚³çµ¦LLMåšæ¨è«–,å‘¼å« OpenAIï¼ˆæˆ–å…¶ä»–æ¨¡å‹ï¼‰é€²è¡Œé æ¸¬,ç”¢å‡ºä¸€å€‹ AIMessage ç‰©ä»¶ï¼ˆä¾‹å¦‚ AIMessage(content="åŠ ç­è²»çš„è¨ˆç®—æ–¹å¼æ˜¯...")ï¼‰ã€‚
    #StrOutputParser()æ¥æ”¶ï¼šAIMessage ç‰©ä»¶å¾ŒæŠŠ content è£¡é¢çš„æ–‡å­—å…§å®¹èƒå–å‡ºä¾†,ç”¢å‡ºå–®ç´”çš„å­—ä¸² (String)ï¼ˆé€™å°±æ˜¯æœ€å¾Œ invoke å›å‚³çš„æ±è¥¿ï¼‰
    rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
    )

    # 6. é–‹å§‹äº’å‹•
    question = "è«‹å•åŠ ç­è²»çš„è¨ˆç®—æ¨™æº–æ˜¯å¦‚ä½•è¦å®šçš„ï¼Ÿ(è«‹å¼•ç”¨æ¢æ–‡)"
    print(f"ğŸ“ æ­£åœ¨è©¢å•å•é¡Œï¼š{question}")
    print("-" * 50)

    # åŸ·è¡Œï¼
    result = rag_chain.invoke(question)

    print("ğŸ’¡ AI å›ç­”çµæœï¼š")
    print(result)
    print("-" * 50)


if __name__ == "__main__":
    run_rag_system()